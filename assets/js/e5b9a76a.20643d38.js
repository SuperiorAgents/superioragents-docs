"use strict";(self.webpackChunksuperior_agents_docs=self.webpackChunksuperior_agents_docs||[]).push([[2245],{275:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/image4-0d28f7294356b96bdc871efb522f0965.png"},316:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/image3-f20c6da27efb839a4c28463558ae9660.png"},6716:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"architecture-design/core-components/intelligence-generalization","title":"Intelligence Generalization","description":"LLMs are known to struggle when it comes to distinguishing fact from hallucination. A part of this can be attributed to the way in which parametric memory (i.e. the compression of data into vectors) works, and a part to the compulsion to be helpful and engaging instilled through reinforcement learning. Mostly, however, it is a reflection of a fundamental property of mathematics: that you can\u2019t effectively check the truth of a statement in a given order formal language if the only tools at your disposal are those provided by that language.","source":"@site/docs/architecture-design/core-components/intelligence-generalization.md","sourceDirName":"architecture-design/core-components","slug":"/architecture-design/core-components/intelligence-generalization","permalink":"/superioragents-docs/docs/architecture-design/core-components/intelligence-generalization","draft":false,"unlisted":false,"editUrl":"https://github.com/superioragents/superior-agents/edit/main/docs/architecture-design/core-components/intelligence-generalization.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Intelligence Generalization"},"sidebar":"tutorialSidebar","previous":{"title":"Intelligence Evaluation","permalink":"/superioragents-docs/docs/architecture-design/core-components/intelligence-evaluation"},"next":{"title":"Experimental Environment","permalink":"/superioragents-docs/docs/architecture-design/core-components/experimental-environment"}}');var o=n(4848),a=n(8453);const s={sidebar_position:3,title:"Intelligence Generalization"},r=void 0,l={},c=[];function h(e){const t={img:"img",li:"li",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.p,{children:"LLMs are known to struggle when it comes to distinguishing fact from hallucination. A part of this can be attributed to the way in which parametric memory (i.e. the compression of data into vectors) works, and a part to the compulsion to be helpful and engaging instilled through reinforcement learning. Mostly, however, it is a reflection of a fundamental property of mathematics: that you can\u2019t effectively check the truth of a statement in a given order formal language if the only tools at your disposal are those provided by that language."}),"\n",(0,o.jsx)(t.p,{children:"To deal with this issue, the principal solution has been to increase the size of the models and the amount of training data used, as well as employing standardized benchmarks (model IQ tests) to assess performance. The problem with this is that if both the training materials and the test are human-produced, the AI is unlikely to ever get much more intelligent than the most intelligent human in any given field[2]. Until we build an AI that can discover and integrate its own knowledge, artificial super-intelligence will remain out of reach."}),"\n",(0,o.jsx)(t.p,{children:"The first approach is to allow the model to develop new ideas and reality-test them itself, as described above. The second, however, is to push the model to develop a higher-order metalanguage for the purpose of establishing a T-schema\u2014that is, an inductive abstraction of truth against which lower-order propositions may be tested. Recent evidence suggests that this is what AI models are attempting to do via the grokking process."}),"\n",(0,o.jsx)(t.p,{children:"Recently, the Grokfast team discovered that memorization and generalization are the product of two separate learning processes\u2014a fast and a slow one. Using some nifty Fourier transformations, they worked out that the adjustments to model weights that happen during training can be decomposed into two frequencies:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"A high frequency signal associated with memorization."}),"\n",(0,o.jsx)(t.li,{children:"A low frequency signal associated with generalization."}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Grokking",src:n(316).A+"",width:"727",height:"267"})}),"\n",(0,o.jsx)(t.p,{children:"The team tested this by amplifying the low frequency signals. The result was vastly accelerated generalization."}),"\n",(0,o.jsx)(t.p,{children:"At roughly the same time, another team in Brazil was trying to work out what exactly the AI is doing when it groks. They suspected that it was trying to work out the relationships between the different clusters of training data it had been given, so they put together a highly curated dataset containing known clusters. Removing a cluster vastly reduced the model\u2019s ability to generalize, while adding just a few examples from that cluster restored it."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Test &amp; Training Accuracy",src:n(275).A+"",width:"412",height:"510"})}),"\n",(0,o.jsx)(t.p,{children:"These results combine to suggest an explanation for what the AI is doing when it groks: specifically, it is trying to adjust its weights with great care and delicacy so as to create patterns that will enable it to encode information regarding the relationships between the groups of data it has been fed, without losing the ability to respond to the original training set. The result is that the model, when asked a question, does not simply spit out an average of the closest bits of rote learning it knows; it compares the likely answer against its own abstraction of answers within that category, coming up with a much better output."}),"\n",(0,o.jsx)(t.p,{children:"This would explain not just the comparatively slow weight change during the grokking process, but also the (otherwise inexplicable) sudden improvement far beyond anything displayed before."}),"\n",(0,o.jsx)(t.p,{children:"Grokking has the additional advantage of requiring relatively little data to attempt. Now that our Superior Agents are live and producing data, our intention is to use this information\u2014which fulfills the high quality/structured data requirements for grokking\u2014to attempt to train a series of models. Transformers\u2014even when grokked\u2014are known to struggle with composition tasks (i.e. ones relating to facts stored in different parts of the model), likely on account of their structure. Consequently, we plan to use diffusion models. These, despite the higher training cost, display grokking-like behavior throughout the normal training process, not simply under specific conditions. Moreover, they over-perform on compositional tasks as well as on coding tasks more generally."}),"\n",(0,o.jsx)(t.p,{children:"This, we feel, is the route to truly independent knowledge generation, and eventually to super-intelligence."})]})}function d(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>r});var i=n(6540);const o={},a=i.createContext(o);function s(e){const t=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);