"use strict";(self.webpackChunksuperior_agents_docs=self.webpackChunksuperior_agents_docs||[]).push([[9556],{877:e=>{e.exports=JSON.parse('{"permalink":"/superioragents-docs/blog/2025-05-15-less-data-more-intelligence","editUrl":"https://github.com/superioragents/superior-agents/edit/main/blog/blog/2025-05-15-less-data-more-intelligence/2025-05-15-less-data-more-intelligence.md","source":"@site/blog/2025-05-15-less-data-more-intelligence/2025-05-15-less-data-more-intelligence.md","title":"Less Data, More Intelligence: How Curated Training Data Unlocks LLM Power","description":"Exploring how carefully curated, minimal datasets can unlock powerful LLM capabilities and reduce training costs.","date":"2025-05-15T00:00:00.000Z","tags":[{"inline":false,"label":"Superior Agents","permalink":"/superioragents-docs/blog/tags/superioragents","description":"Superior Agents tag description"},{"inline":false,"label":"Research","permalink":"/superioragents-docs/blog/tags/research","description":"Research tag description"}],"readingTime":3.65,"hasTruncateMarker":true,"authors":[{"name":"Jen D.","title":"Chief AI Engineer @ KIP","page":{"permalink":"/superioragents-docs/blog/authors/jen"},"socials":{"x":"https://x.com/HumanLevelJen"},"imageURL":"https://pbs.twimg.com/profile_images/1917918976488747008/kWobzLPQ_400x400.jpg","key":"jen"}],"frontMatter":{"slug":"2025-05-15-less-data-more-intelligence","title":"Less Data, More Intelligence: How Curated Training Data Unlocks LLM Power","authors":["jen"],"tags":["Superior Agents","research"]},"unlisted":false,"prevItem":{"title":"Adapting Diffusion Models for No-Propagation Learning","permalink":"/superioragents-docs/blog/2025-05-15-adapting-diffusion-models"},"nextItem":{"title":"What\'s Next: No supervision. No benchmarks. No limits.","permalink":"/superioragents-docs/blog/2025-04-25-superior-agents"}}')},6283:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var s=n(877),i=n(4848),r=n(8453);const a={slug:"2025-05-15-less-data-more-intelligence",title:"Less Data, More Intelligence: How Curated Training Data Unlocks LLM Power",authors:["jen"],tags:["Superior Agents","research"]},o=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Introduction",id:"introduction",level:2},{value:"\u201cLess Is More\u201d \u2013 Complex Reasoning from a Tiny Dataset",id:"less-is-more--complex-reasoning-from-a-tiny-dataset",level:2},{value:"Quality Over Quantity: Data Efficiency in Practice",id:"quality-over-quantity-data-efficiency-in-practice",level:2},{value:"Few-Shot Learning versus Small-Batch Fine-Tuning",id:"few-shot-learning-versus-small-batch-fine-tuning",level:2},{value:"Strategies for Training LLMs with Limited Data",id:"strategies-for-training-llms-with-limited-data",level:2},{value:"Implications: Cost, Scarcity, Ethics",id:"implications-cost-scarcity-ethics",level:2},{value:"Future Directions",id:"future-directions",level:2}];function d(e){const t={a:"a",br:"br",em:"em",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.p,{children:(0,i.jsx)(t.em,{children:"Exploring how carefully curated, minimal datasets can unlock powerful LLM capabilities and reduce training costs."})}),"\n",(0,i.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(t.p,{children:["Training large language models (LLMs) has traditionally been a game of scale\u2014ingesting massive datasets to coax out higher performance. However, a growing body of research shows that ",(0,i.jsx)(t.strong,{children:"what"})," data you use can matter more than ",(0,i.jsx)(t.strong,{children:"how much"}),".",(0,i.jsx)(t.br,{}),"\n","A 2025 study from Shanghai Jiao Tong University revealed that a 32-billion-parameter model fine-tuned on only ",(0,i.jsx)(t.strong,{children:"817 carefully selected examples"})," learned complex mathematical-reasoning tasks\u2014matching or beating models trained on ",(0,i.jsx)(t.strong,{children:"hundreds of thousands"})," of examples (",(0,i.jsx)(t.a,{href:"https://venturebeat.com/ai/researchers-find-you-dont-need-a-ton-of-data-to-train-llms-for-reasoning-tasks/",children:"VentureBeat report"}),")."]}),"\n",(0,i.jsx)(t.p,{children:"This article explains how small, high-quality datasets dramatically improve LLMs, surveys other work on data efficiency and few-shot learning, and outlines techniques for \u201cdoing more with less\u201d\u2014from judicious repetition and code augmentation to relaxed data filters. We also discuss cost, data-scarcity, and ethical implications, and highlight future directions for data-efficient LLM training."}),"\n",(0,i.jsx)(t.h2,{id:"less-is-more--complex-reasoning-from-a-tiny-dataset",children:"\u201cLess Is More\u201d \u2013 Complex Reasoning from a Tiny Dataset"}),"\n",(0,i.jsxs)(t.p,{children:["The \u201c",(0,i.jsx)(t.strong,{children:"LIMO"}),"\u201d (Less Is More) study fine-tuned Qwen-2.5 on just 817 expert-curated math problems yet hit ",(0,i.jsx)(t.strong,{children:"57.1 %"})," on the AIME competition and ",(0,i.jsx)(t.strong,{children:"94.8 %"})," on MATH\u2014results that previously required 100\xd7 more data (",(0,i.jsx)(t.a,{href:"https://ar5iv.org/abs/2401.11405",children:"ar5iv preprint"}),").",(0,i.jsx)(t.br,{}),"\n","Key to LIMO\u2019s success was an aggressive curation pipeline that sifted millions of synthetic problems down to \u201ccognitive templates\u201d demonstrating step-by-step chain-of-thought solutions. Because modern LLMs already possess vast latent knowledge from pre-training, a handful of exemplars can unlock sophisticated reasoning skills."]}),"\n",(0,i.jsxs)(t.p,{children:["This echoes Meta AI\u2019s ",(0,i.jsx)(t.strong,{children:"LIMA"})," (\u201cLess Is More for Alignment\u201d) work, where 1 000 high-quality prompt-response pairs tuned a large model to human-preferred outputs as reliably as datasets orders of magnitude larger (",(0,i.jsx)(t.a,{href:"https://huggingface.co/blog/lima",children:"Hugging Face blog"}),"). Together, LIMO and LIMA underscore a simple principle: ",(0,i.jsx)(t.strong,{children:"a small batch of the right data beats a mountain of mediocre data"}),"."]}),"\n",(0,i.jsx)(t.h2,{id:"quality-over-quantity-data-efficiency-in-practice",children:"Quality Over Quantity: Data Efficiency in Practice"}),"\n",(0,i.jsxs)(t.p,{children:["Microsoft Research\u2019s ",(0,i.jsx)(t.strong,{children:"Phi-1"})," (\u201cTextbooks Are All You Need\u201d) trained a 1.3 B model on ",(0,i.jsx)(t.strong,{children:"6 B tokens"})," of textbook-style text and code\u2014outperforming models 10\xd7 larger trained on 100\xd7 more tokens (",(0,i.jsx)(t.a,{href:"https://medium.com/@msr_research/phi-1-textbooks-are-all-you-need-4f4314d7242e",children:"Phi-1 write-up"}),").",(0,i.jsx)(t.br,{}),"\n","Empirical tests confirm that halving dataset ",(0,i.jsx)(t.strong,{children:"quality"})," harms performance more than halving its ",(0,i.jsx)(t.strong,{children:"size"}),"; modest duplication (\u224825 % repeat) can even help, whereas indiscriminate repetition quickly overfits (",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2305.07759",children:"TinyStories study, arXiv"}),")."]}),"\n",(0,i.jsxs)(t.p,{children:["Training on endless web text is costly and energy-intensive. Phi-1\u2019s entire run reportedly cost ",(0,i.jsx)(t.strong,{children:"< $1 200"}),", while LIMO\u2019s 817-sample fine-tune is feasible for small labs. Crucially, curation assures ",(0,i.jsx)(t.strong,{children:"coverage"}),": LIMO\u2019s examples span diverse techniques and difficulties; LIMA\u2019s prompts cover many domains and styles. High-coverage, high-quality micro-datasets punch far above their weight."]}),"\n",(0,i.jsx)(t.h2,{id:"few-shot-learning-versus-small-batch-fine-tuning",children:"Few-Shot Learning versus Small-Batch Fine-Tuning"}),"\n",(0,i.jsxs)(t.p,{children:["GPT-3 popularised ",(0,i.jsx)(t.strong,{children:"in-context few-shot learning"}),"\u2014solving new tasks from a few prompt examples. Yet prompts are length-limited and results can vary. ",(0,i.jsx)(t.strong,{children:"Small-batch fine-tuning"})," (\u2264 1 000 examples) permanently aligns model weights and often outperforms prompting."]}),"\n",(0,i.jsxs)(t.p,{children:["Methods like ",(0,i.jsx)(t.strong,{children:"LoRA"})," update only low-rank adapters, preventing over-fitting in scarce-data regimes. LIMO fine-tuned Qwen-2.5 on chain-of-thought exemplars so it now ",(0,i.jsx)(t.em,{children:"generates"})," its own reasoning steps for unseen problems\u2014no long prompts required."]}),"\n",(0,i.jsx)(t.h2,{id:"strategies-for-training-llms-with-limited-data",children:"Strategies for Training LLMs with Limited Data"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Curate High-Quality Examples"})," \u2013 correctness, diversity, task coverage."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Moderate Repetition & Augmentation"})," \u2013 light duplication or paraphrases boost signal; avoid heavy overfit."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Include Code / Structured Data"})," \u2013 mixing a little high-quality code improves logic and planning skills (",(0,i.jsx)(t.a,{href:"https://notes.aimodels.fyi/code-training-benefits",children:"Cohere findings"}),")."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Relax Filters Judiciously"})," \u2013 when data-starved, keep borderline but relevant samples (after manual review)."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Generate Synthetic Data"})," \u2013 projects like ",(0,i.jsx)(t.strong,{children:"Alpaca"})," expanded 52 K instruction pairs from a handful of seeds using GPT-3.5 (",(0,i.jsx)(t.a,{href:"https://crfm.stanford.edu/2023/03/13/alpaca.html",children:"Stanford post"}),")."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"implications-cost-scarcity-ethics",children:"Implications: Cost, Scarcity, Ethics"}),"\n",(0,i.jsxs)(t.p,{children:["Data-efficient training ",(0,i.jsx)(t.strong,{children:"democratises"})," LLM development\u2014start-ups and academics can fine-tune potent models for a few hundred dollars. As high-quality web text dwindles, smart data use will extend progress without vast new crawls."]}),"\n",(0,i.jsxs)(t.p,{children:["Curated micro-datasets also enable tighter ",(0,i.jsx)(t.strong,{children:"ethical control"}),"\u2014easier to exclude toxic or private content and audit for bias. But selection bias rises: a tiny set may over-represent curators\u2019 viewpoints. Privacy risks increase when fine-tuning on very small sensitive corpora; techniques like differential privacy or federated learning help mitigate memorisation."]}),"\n",(0,i.jsx)(t.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Automated Data Selection"})," \u2013 frameworks like GREATS rank candidate examples for maximal learning (",(0,i.jsx)(t.a,{href:"https://openreview.net/forum?id=Wk05uJwYjX",children:"OpenReview"}),")."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Hybrid RL + Tiny Supervision"})," \u2013 seed with curated data, then self-play or AI-generated challenges to scale."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Teacher-Student Distillation"})," \u2013 larger models create compact, information-dense \u201ctextbooks\u201d for smaller learners."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Bias/Privacy Auditing Tools"})," \u2013 essential when each example wields outsized influence."]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["The age of ",(0,i.jsx)(t.strong,{children:"data craftsmanship"})," is here: less, done right, truly can be more."]})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>o});var s=n(6540);const i={},r=s.createContext(i);function a(e){const t=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(r.Provider,{value:t},e.children)}}}]);