<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Defining T-Schemas via the Parametric Encoding of Second Order Languages in AI Models | Superior Agents Docs</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://SuperiorAgents.github.io/superioragents-docs/img/superior_agents_logo.jpg"><meta data-rh="true" name="twitter:image" content="https://SuperiorAgents.github.io/superioragents-docs/img/superior_agents_logo.jpg"><meta data-rh="true" property="og:url" content="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-03-19-superior-agents"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Defining T-Schemas via the Parametric Encoding of Second Order Languages in AI Models | Superior Agents Docs"><meta data-rh="true" name="description" content="In this short article, we present a summary of current work on the grokking phenomenon that emerges when AI models are significantly over-trained. We suggest that this provides evidence of the model&#x27;s attempts to define truth inductively through the creation of consensus sets within the base training set and encode it via patterns overlaid upon the same parameters used to memorize this set."><meta data-rh="true" property="og:description" content="In this short article, we present a summary of current work on the grokking phenomenon that emerges when AI models are significantly over-trained. We suggest that this provides evidence of the model&#x27;s attempts to define truth inductively through the creation of consensus sets within the base training set and encode it via patterns overlaid upon the same parameters used to memorize this set."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-03-19T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="Superior Agents"><link data-rh="true" rel="icon" href="/superioragents-docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-03-19-superior-agents"><link data-rh="true" rel="alternate" href="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-03-19-superior-agents" hreflang="en"><link data-rh="true" rel="alternate" href="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-03-19-superior-agents" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://SuperiorAgents.github.io/superioragents-docs/blog/2025-03-19-superior-agents","mainEntityOfPage":"https://SuperiorAgents.github.io/superioragents-docs/blog/2025-03-19-superior-agents","url":"https://SuperiorAgents.github.io/superioragents-docs/blog/2025-03-19-superior-agents","headline":"Defining T-Schemas via the Parametric Encoding of Second Order Languages in AI Models","name":"Defining T-Schemas via the Parametric Encoding of Second Order Languages in AI Models","description":"In this short article, we present a summary of current work on the grokking phenomenon that emerges when AI models are significantly over-trained. We suggest that this provides evidence of the model's attempts to define truth inductively through the creation of consensus sets within the base training set and encode it via patterns overlaid upon the same parameters used to memorize this set.","datePublished":"2025-03-19T00:00:00.000Z","author":{"@type":"Person","name":"Jen D.","description":"Chief AI Engineer @ KIP","image":"https://pbs.twimg.com/profile_images/1917918976488747008/kWobzLPQ_400x400.jpg"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://SuperiorAgents.github.io/superioragents-docs/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/superioragents-docs/blog/rss.xml" title="Superior Agents Docs RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/superioragents-docs/blog/atom.xml" title="Superior Agents Docs Atom Feed"><link rel="stylesheet" href="/superioragents-docs/assets/css/styles.f41048b3.css">
<script src="/superioragents-docs/assets/js/runtime~main.78f3ba60.js" defer="defer"></script>
<script src="/superioragents-docs/assets/js/main.02f5090c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/superioragents-docs/"><div class="navbar__logo"><img src="/superioragents-docs/img/superior_agents_logo.jpg" alt="Superior Agents Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/superioragents-docs/img/superior_agents_logo.jpg" alt="Superior Agents Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Superior Agents Docs</b></a><a class="navbar__item navbar__link" href="/superioragents-docs/docs/category/getting-started">Documentation</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/superioragents-docs/blog">Blogs</a><a class="navbar__item navbar__link" href="/superioragents-docs/research">Research</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/SuperiorAgents/superioragents-docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_YFbd" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/superioragents-docs/blog/2025-05-15-adapting-diffusion-models">Adapting Diffusion Models for No-Propagation Learning</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/superioragents-docs/blog/2025-05-15-less-data-more-intelligence">Less Data, More Intelligence: How Curated Training Data Unlocks LLM Power</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/superioragents-docs/blog/2025-04-25-superior-agents">What&#x27;s Next: No supervision. No benchmarks. No limits.</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/superioragents-docs/blog/2025-04-23-superior-agents">Superior Agents Reimagining AI with Darwinian Self-Improvement</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/superioragents-docs/blog/2025-03-19-superior-agents">Defining T-Schemas via the Parametric Encoding of Second Order Languages in AI Models</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">Defining T-Schemas via the Parametric Encoding of Second Order Languages in AI Models</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-03-19T00:00:00.000Z">March 19, 2025</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/superioragents-docs/blog/authors/jen"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1917918976488747008/kWobzLPQ_400x400.jpg" alt="Jen D."></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/superioragents-docs/blog/authors/jen"><span class="authorName_yefp">Jen D.</span></a></div><small class="authorTitle_nd0D" title="Chief AI Engineer @ KIP">Chief AI Engineer @ KIP</small><div class="authorSocials_rSDt"><a href="https://x.com/HumanLevelJen" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="X"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="none" viewBox="0 0 1200 1227" style="--dark:#000;--light:#fff" class="authorSocialLink_owbf xSvg_y3PF"><path d="M714.163 519.284 1160.89 0h-105.86L667.137 450.887 357.328 0H0l468.492 681.821L0 1226.37h105.866l409.625-476.152 327.181 476.152H1200L714.137 519.284h.026ZM569.165 687.828l-47.468-67.894-377.686-540.24h162.604l304.797 435.991 47.468 67.894 396.2 566.721H892.476L569.165 687.854v-.026Z"></path></svg></a></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>In this short article, we present a summary of current work on the grokking phenomenon that emerges when AI models are significantly over-trained. We suggest that this provides evidence of the model&#x27;s attempts to define truth inductively through the creation of consensus sets within the base training set and encode it via patterns overlaid upon the same parameters used to memorize this set.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="can-an-ai-tell-the-truth">Can an AI Tell the Truth?<a href="#can-an-ai-tell-the-truth" class="hash-link" aria-label="Direct link to Can an AI Tell the Truth?" title="Direct link to Can an AI Tell the Truth?">​</a></h2>
<p>In order to improve, it is necessary to know whether what you&#x27;re doing now is right or wrong. For AI models this is extremely difficult. LLMs are known to struggle when it comes to distinguishing fact and hallucination. A part of this can be attributed to the way in which parametric memory (i.e. the compression of data into vectors) works, and a part to the compulsion to be helpful and engaging instilled through reinforcement learning. Mostly, however it is a reflection of a fundamental property of mathematics: that you can&#x27;t effectively check the truth of a statement in a given order formal language if the only tools at your disposal are those provided by that language. Or, alternatively:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="undefinability-theorem-"><img decoding="async" loading="lazy" alt="Undefinability Theorem " src="/superioragents-docs/assets/images/1-c2daf31ad05f578995b77522f1ef6de2.jpg" width="721" height="84" class="img_ev3q"><a href="#undefinability-theorem-" class="hash-link" aria-label="Direct link to undefinability-theorem-" title="Direct link to undefinability-theorem-">​</a></h3>
<blockquote>
<p>In other words, there is no way to prove that 1+1=2 using basic arithmetic alone<sup><a href="#user-content-fn-1-3ebd1a" id="user-content-fnref-1-3ebd1a" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>.</p>
</blockquote>
<p>Operating as it does at a single layer of abstraction, an AI model is incapable of saying that 1+1=328 is definitely wrong; it can only say that it is improbable. To deal with this issue, the principal solution has been to increase the size of models and the amount of training data used, as well as employing standardized benchmarks (model IQ tests) to assess performance.</p>
<p>The problem with this approach is that if both the training materials and the test are human-produced, the AI is unlikely to ever get much more intelligent than the most intelligent human in any given field<sup><a href="#user-content-fn-2-3ebd1a" id="user-content-fnref-2-3ebd1a" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>. Until we build an AI that can discover and integrate its own knowledge, artificial super-intelligence will remain out of reach.</p>
<p>But if the only way to learn new things is from smarter humans, then how did the smartest human do it?</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-work-arounds">The Work-Arounds<a href="#the-work-arounds" class="hash-link" aria-label="Direct link to The Work-Arounds" title="Direct link to The Work-Arounds">​</a></h2>
<p>Fortunately, there are two ways around this problem. The first is <strong>reality testing</strong>. I couldn&#x27;t employ first-order arithmetic to prove that 1+1=2, but I just have to hold up two fingers to know it to be true. Not having contact with the physical world, it is harder for LLMs to do this, but not impossible—give an AI an enumerable, extensible, and objective metric that is outside its control (such as the amount of storage space taken up by its own backups or the amount of money in a crypto wallet) and the coding tools to interact with the world in such a way as to affect this metric, and it will begin reality-testing hypotheses immediately. If the status of successful and failed attempts is then stored for future fine-tuning, the model becomes capable of self-improvement (as we have demonstrated <a href="https://xianyangcb.substack.com/p/a-system-for-evolving-general-artificial-intelligence-from-existing-technologies-b4f5c4d1335a" target="_blank" rel="noopener noreferrer">elsewhere</a>).</p>
<p>Under this approach, no smarter human is required: the model comes up with its own ideas, tests them, and learns which work and which don&#x27;t from its own testing. After all, when Newton developed his theory of gravitation, he checked it against observed planetary movements, not against the opinions of someone more intelligent.</p>
<p>But he did something else as well, which brings us to the <strong>second work-around</strong>: he created a higher-order metalanguage for the purpose of establishing a <a href="https://en.wikipedia.org/wiki/T-schema" target="_blank" rel="noopener noreferrer">T-schema</a>—that is, an inductive abstraction of truth against which lower-order propositions may be tested. Based on empirical data concerning planetary movements (or at least Kepler&#x27;s data), he derived a set of formulae that succinctly explained all of these movements. From that point on, if the formula says that Mercury should be 36 million miles from the sun, while your latest observation suggests that it is only 24 miles away, then the likelihood is that your observation is wrong—after all, the formula can cite every single previous observation to back it up.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="truth-through-consensus">Truth Through Consensus<a href="#truth-through-consensus" class="hash-link" aria-label="Direct link to Truth Through Consensus" title="Direct link to Truth Through Consensus">​</a></h2>
<p>In this article we suggest that recent evidence from several papers suggests that this is what AI models are attempting to do via the <strong>grokking</strong> process.</p>
<p><strong>Grokking</strong> refers to a phenomenon where AI models&#x27; test responses improve with more training, grow worse as they pass the point of over-training (i.e., when they become too focused on memorizing the training set and lose the ability to generalize), and then suddenly improve to an astonishing degree.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="modular-divison"><img decoding="async" loading="lazy" alt="Modular Divison" src="/superioragents-docs/assets/images/2-11274269013d750543b31a6db7c7df99.png" width="1062" height="312" class="img_ev3q"><a href="#modular-divison" class="hash-link" aria-label="Direct link to modular-divison" title="Direct link to modular-divison">​</a></h3>
<p>As seen in the third image, the model&#x27;s answers during training improve as it memorizes the training questions with greater fidelity, leading to a continuous decrease in training loss. However, test performance initially improves, then declines as the model overfits to the training set. Finally, as the model groks, <strong>test loss drops to near zero</strong>, indicating successful generalization. These grokked models can be remarkably powerful— <a href="https://arxiv.org/pdf/2405.150712" target="_blank" rel="noopener noreferrer">one team</a> even managed to outperform GPT-4 Turbo and Gemini 1.5 using a grokked GPT<sup><a href="#user-content-fn-3-3ebd1a" id="user-content-fnref-3-3ebd1a" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup>.</p>
<p>Recently, the <a href="https://arxiv.org/pdf/2405.20233" target="_blank" rel="noopener noreferrer">Grokfast team</a> discovered that <strong>memorization and generalization are driven by two distinct learning processes</strong>, a fast and a slow one. Using Fourier transformations, they determined that adjustments to model weights during training can be decomposed into <strong>high-frequency signals</strong> (associated with memorization) and <strong>low-frequency signals</strong> (associated with generalization). By amplifying the low-frequency signals, they vastly accelerated generalization.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="memorization-and-generalization"><img decoding="async" loading="lazy" alt="Memorization and Generalization" src="/superioragents-docs/assets/images/3-bf7f564593c49783d0c3e9874051e95f.png" width="727" height="267" class="img_ev3q"><a href="#memorization-and-generalization" class="hash-link" aria-label="Direct link to memorization-and-generalization" title="Direct link to memorization-and-generalization">​</a></h3>
<p>At roughly the same time, a <a href="https://arxiv.org/abs/2409.08282" target="_blank" rel="noopener noreferrer">research team in Brazil</a> explored what happens when an AI groks. They suspected that the model was <strong>identifying relationships between different clusters of training data</strong>, so they curated a dataset with known clusters. Removing a cluster drastically reduced generalization, while adding just a few examples from that cluster restored it.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="accuracy"><img decoding="async" loading="lazy" alt="Accuracy" src="/superioragents-docs/assets/images/4-5fc91bf1f474186cee28f73a9388cb53.png" width="412" height="510" class="img_ev3q"><a href="#accuracy" class="hash-link" aria-label="Direct link to accuracy" title="Direct link to accuracy">​</a></h3>
<p>These findings suggest that when an AI <strong>groks</strong>, it carefully adjusts its weights to encode information about relationships between groups of data while preserving its ability to recall the original training set. This delicate adjustment process explains why grokking takes so long.</p>
<p>To enable this, it is obliged to develop an internal metalanguage that will allow it to conduct internal evaluations of the &quot;truthiness&quot; of any given input statement based upon its abstraction of similar statements received previously. If every flower in the cat&#x27;s eyes is blue or purple, then an input suggesting that it should place a yellow one there will be flagged up as likely incorrect and checked. The result is that the model, when asked a question, does not simply spit out an average of the closest bits of rote learning it knows, it compares the likely answer against its own abstraction of answers within that category, coming up with a much better output. Or, as <a href="https://arxiv.org/pdf/2405.15071" target="_blank" rel="noopener noreferrer">Wang et al.</a> put it:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="wang-et-al"><img decoding="async" loading="lazy" alt="Wang et al." src="/superioragents-docs/assets/images/Wang-article-d7176b4e72d60c1a058921de5eb46106.png" width="691" height="330" class="img_ev3q"><a href="#wang-et-al" class="hash-link" aria-label="Direct link to wang-et-al" title="Direct link to wang-et-al">​</a></h3>
<p>This would explain both the <strong>slow weight change</strong> during the grokking process and the <strong>sudden leap in performance</strong> once it generalizes. Further evidence from <a href="https://openreview.net/pdf?id=6NHnsjsYXH" target="_blank" rel="noopener noreferrer">Yunis et al</a> suggests that grokking is associated with the model discovering a <strong>low-rank encoding solution</strong> (i.e., one that multiplies smaller-than-normal matrices):</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="grokking-rank-minimization--generalization"><img decoding="async" loading="lazy" alt="Grokking, Rank Minimization &amp;amp; Generalization" src="/superioragents-docs/assets/images/5-bb7aed4866287edb0937441eedaa8445.png" width="678" height="860" class="img_ev3q"><a href="#grokking-rank-minimization--generalization" class="hash-link" aria-label="Direct link to grokking-rank-minimization--generalization" title="Direct link to grokking-rank-minimization--generalization">​</a></h3>
<p>This, in turn, implies that the model is suddenly achieving higher performance as a consequence of encoding fewer features. The only possible explanation for this is that it has developed its own abstraction layer in which to talk to itself about higher order truths.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="building-upon-these-results">Building Upon These Results<a href="#building-upon-these-results" class="hash-link" aria-label="Direct link to Building Upon These Results" title="Direct link to Building Upon These Results">​</a></h2>
<p>Grokking has the additional advantage of requiring relatively little data to attempt. Now that our <a href="https://xianyangcb.substack.com/p/artificial-intelligences-in-the-guanzi" target="_blank" rel="noopener noreferrer">Generalising Agents</a> are up and producing data, our intention is to use this information - which fulfills the high quality/structured data requirements for grokking - to attempt to train a series of models. Transformers - even when grokked - are <a href="https://arxiv.org/pdf/2405.15071" target="_blank" rel="noopener noreferrer">known to struggle</a> with composition tasks (i.e. ones relating to facts stored in different parts of the model), likely on account of their structure. Consequently, we plan to use diffusion models. These, despite the higher training cost, display <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/9d0f188c7947eacb0c07f709576824f6-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">grokking-like behaviour</a> throughout the normal training process, not simply under specific conditions. Moreover, they over-perform on <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/9d0f188c7947eacb0c07f709576824f6-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">compsitional tasks</a> as well as on <a href="https://arxiv.org/pdf/2410.17891" target="_blank" rel="noopener noreferrer">coding tasks more generally</a> <sup><a href="#user-content-fn-4-3ebd1a" id="user-content-fnref-4-3ebd1a" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup>, most likely due to their multiplicative approach to learning, as described <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/9d0f188c7947eacb0c07f709576824f6-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">here</a>. This is interesting from our perspective, given that we predicted something similar and made it a foundational component of our <a href="https://xianyangcb.substack.com/p/a-system-for-evolving-general-artificial-intelligence-from-existing-technologies-b4f5c4d1335a" target="_blank" rel="noopener noreferrer">Generalising Agent</a> schema in 2020.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="capabilities--compositionality"><img decoding="async" loading="lazy" alt="Capabilities &amp;amp; Compositionality" src="/superioragents-docs/assets/images/capabilities-3d00bb135c080d85942b17689a2b000b.png" width="848" height="367" class="img_ev3q"><a href="#capabilities--compositionality" class="hash-link" aria-label="Direct link to capabilities--compositionality" title="Direct link to capabilities--compositionality">​</a></h3>
<p>This approach, in theory, could produce models capable of <strong>discovering new real-world knowledge</strong> and <strong>generalizing relationships between data points</strong>. We believe this is the path to <strong>true independent knowledge generation</strong>, and eventually, to <strong>super-intelligence</strong>.</p>
<hr>
<!-- -->
<section data-footnotes="true" class="footnotes"><h2 class="anchor anchorWithStickyNavbar_LWe7 sr-only" id="footnote-label">Footnotes<a href="#footnote-label" class="hash-link" aria-label="Direct link to Footnotes" title="Direct link to Footnotes">​</a></h2>
<ol>
<li id="user-content-fn-1-3ebd1a">
<p>Even with extended formal logic, proving fundamental arithmetic truths is complex—Bertrand Russell and Alfred North Whitehead required hundreds of pages to establish basic mathematical principles. <img decoding="async" loading="lazy" alt="Symbolic Logic" src="/superioragents-docs/assets/images/6-bf4335cb6132e47dc48b6d76fadc2f81.png" width="800" height="333" class="img_ev3q"> <a href="#user-content-fnref-1-3ebd1a" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2-3ebd1a">
<p>Or, in some cases, only a mediocre human—many benchmark datasets contain serious errors, including over 50% of virology problems in MMLU. <img decoding="async" loading="lazy" alt="MMLU Redux Subjects" src="/superioragents-docs/assets/images/7-fcb2e7c38f11ed8c0b22cd2383f626ea.png" width="1248" height="451" class="img_ev3q"> <a href="#user-content-fnref-2-3ebd1a" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3-3ebd1a">
<p>Grokking has not been systematically applied to large models due to cost and risk, though Mark Zuckerberg hinted that Meta may have experimented with it internally. <a href="#user-content-fnref-3-3ebd1a" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4-3ebd1a">
<p>Diffusion models&#x27; superior performance in composition tasks suggests a connection between their structure and grokking behavior. <img decoding="async" loading="lazy" alt="Diffusion Models" src="/superioragents-docs/assets/images/8-6c05aa1b584db5f430a9c2b4e3558c91.png" width="603" height="317" class="img_ev3q"> <a href="#user-content-fnref-4-3ebd1a" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a title="Superior Agents tag description" class="tag_zVej tagRegular_sFm0" href="/superioragents-docs/blog/tags/superioragents">Superior Agents</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/superioragents/superior-agents/edit/main/blog/blog/2025-03-19-superior-agents/2025-03-19-superior-agents.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/superioragents-docs/blog/2025-04-23-superior-agents"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">Superior Agents Reimagining AI with Darwinian Self-Improvement</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/superioragents-docs/blog/2025-01-28-superior-agents"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">System for Evolving AGI from Existing Technologies</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#can-an-ai-tell-the-truth" class="table-of-contents__link toc-highlight">Can an AI Tell the Truth?</a><ul><li><a href="#undefinability-theorem-" class="table-of-contents__link toc-highlight">Undefinability Theorem </a></li></ul></li><li><a href="#the-work-arounds" class="table-of-contents__link toc-highlight">The Work-Arounds</a></li><li><a href="#truth-through-consensus" class="table-of-contents__link toc-highlight">Truth Through Consensus</a><ul><li><a href="#modular-divison" class="table-of-contents__link toc-highlight">Modular Divison</a></li><li><a href="#memorization-and-generalization" class="table-of-contents__link toc-highlight">Memorization and Generalization</a></li><li><a href="#accuracy" class="table-of-contents__link toc-highlight">Accuracy</a></li><li><a href="#wang-et-al" class="table-of-contents__link toc-highlight">Wang et al.</a></li><li><a href="#grokking-rank-minimization--generalization" class="table-of-contents__link toc-highlight">Grokking, Rank Minimization & Generalization</a></li></ul></li><li><a href="#building-upon-these-results" class="table-of-contents__link toc-highlight">Building Upon These Results</a><ul><li><a href="#capabilities--compositionality" class="table-of-contents__link toc-highlight">Capabilities & Compositionality</a></li></ul></li></ul></div></div></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/superioragents-docs/docs/category/getting-started">Documentation</a></li><li class="footer__item"><a href="https://arxiv.org/abs/2504.04711" target="_blank" rel="noopener noreferrer" class="footer__link-item">arXiv Research Paper<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.gg/865JrDPU2J" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://x.com/Superior_Agents" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/superioragents-docs/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/SuperiorAgents/superior-agents" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 KIP Protocol</div></div></div></footer></div>
</body>
</html>