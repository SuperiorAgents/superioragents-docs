<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Adapting Diffusion Models for No-Propagation Learning | Superior Agents Docs</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://SuperiorAgents.github.io/superioragents-docs/img/superior_agents_logo.jpg"><meta data-rh="true" name="twitter:image" content="https://SuperiorAgents.github.io/superioragents-docs/img/superior_agents_logo.jpg"><meta data-rh="true" property="og:url" content="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-05-15-adapting-diffusion-models"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Adapting Diffusion Models for No-Propagation Learning | Superior Agents Docs"><meta data-rh="true" name="description" content="A comprehensive report on extending backprop-free “NoProp” diffusion training from image classification to text generation, covering state-of-the-art diffusion LMs, alternative learning rules, challenges, and research directions."><meta data-rh="true" property="og:description" content="A comprehensive report on extending backprop-free “NoProp” diffusion training from image classification to text generation, covering state-of-the-art diffusion LMs, alternative learning rules, challenges, and research directions."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-05-15T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="Superior Agents,Research"><link data-rh="true" rel="icon" href="/superioragents-docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-05-15-adapting-diffusion-models"><link data-rh="true" rel="alternate" href="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-05-15-adapting-diffusion-models" hreflang="en"><link data-rh="true" rel="alternate" href="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-05-15-adapting-diffusion-models" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://SuperiorAgents.github.io/superioragents-docs/blog/2025-05-15-adapting-diffusion-models","mainEntityOfPage":"https://SuperiorAgents.github.io/superioragents-docs/blog/2025-05-15-adapting-diffusion-models","url":"https://SuperiorAgents.github.io/superioragents-docs/blog/2025-05-15-adapting-diffusion-models","headline":"Adapting Diffusion Models for No-Propagation Learning","name":"Adapting Diffusion Models for No-Propagation Learning","description":"A comprehensive report on extending backprop-free “NoProp” diffusion training from image classification to text generation, covering state-of-the-art diffusion LMs, alternative learning rules, challenges, and research directions.","datePublished":"2025-05-15T00:00:00.000Z","author":{"@type":"Person","name":"Jen D.","description":"Chief AI Engineer @ KIP","image":"https://pbs.twimg.com/profile_images/1917918976488747008/kWobzLPQ_400x400.jpg"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://SuperiorAgents.github.io/superioragents-docs/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/superioragents-docs/blog/rss.xml" title="Superior Agents Docs RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/superioragents-docs/blog/atom.xml" title="Superior Agents Docs Atom Feed"><link rel="stylesheet" href="/superioragents-docs/assets/css/styles.f41048b3.css">
<script src="/superioragents-docs/assets/js/runtime~main.78f3ba60.js" defer="defer"></script>
<script src="/superioragents-docs/assets/js/main.02f5090c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/superioragents-docs/"><div class="navbar__logo"><img src="/superioragents-docs/img/superior_agents_logo.jpg" alt="Superior Agents Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/superioragents-docs/img/superior_agents_logo.jpg" alt="Superior Agents Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Superior Agents Docs</b></a><a class="navbar__item navbar__link" href="/superioragents-docs/docs/category/getting-started">Documentation</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/superioragents-docs/blog">Blogs</a><a class="navbar__item navbar__link" href="/superioragents-docs/research">Research</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/SuperiorAgents/superioragents-docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_YFbd" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/superioragents-docs/blog/2025-05-15-adapting-diffusion-models">Adapting Diffusion Models for No-Propagation Learning</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/superioragents-docs/blog/2025-05-15-less-data-more-intelligence">Less Data, More Intelligence: How Curated Training Data Unlocks LLM Power</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/superioragents-docs/blog/2025-04-25-superior-agents">What&#x27;s Next: No supervision. No benchmarks. No limits.</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/superioragents-docs/blog/2025-04-23-superior-agents">Superior Agents Reimagining AI with Darwinian Self-Improvement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/superioragents-docs/blog/2025-03-19-superior-agents">Defining T-Schemas via the Parametric Encoding of Second Order Languages in AI Models</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">Adapting Diffusion Models for No-Propagation Learning</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-05-15T00:00:00.000Z">May 15, 2025</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/superioragents-docs/blog/authors/jen"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1917918976488747008/kWobzLPQ_400x400.jpg" alt="Jen D."></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/superioragents-docs/blog/authors/jen"><span class="authorName_yefp">Jen D.</span></a></div><small class="authorTitle_nd0D" title="Chief AI Engineer @ KIP">Chief AI Engineer @ KIP</small><div class="authorSocials_rSDt"><a href="https://x.com/HumanLevelJen" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="X"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="none" viewBox="0 0 1200 1227" style="--dark:#000;--light:#fff" class="authorSocialLink_owbf xSvg_y3PF"><path d="M714.163 519.284 1160.89 0h-105.86L667.137 450.887 357.328 0H0l468.492 681.821L0 1226.37h105.866l409.625-476.152 327.181 476.152H1200L714.137 519.284h.026ZM569.165 687.828l-47.468-67.894-377.686-540.24h162.604l304.797 435.991 47.468 67.894 396.2 566.721H892.476L569.165 687.854v-.026Z"></path></svg></a></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p><em>A comprehensive report on extending backprop-free “NoProp” diffusion training from image classification to text generation, covering state-of-the-art diffusion LMs, alternative learning rules, challenges, and research directions.</em></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<p>Recent research has begun exploring backpropagation-free learning methods as an alternative to the traditional gradient-descent/backpropagation algorithm. The motivation includes reducing memory overhead (since back-prop requires storing activations) and enabling more parallel or biologically plausible training (<a href="https://medium.com" target="_blank" rel="noopener noreferrer">medium.com</a>). One promising avenue is to leverage ideas from <strong>diffusion models</strong>—generative models that iteratively refine noisy data—to train networks <strong>without</strong> standard error back-propagation.</p>
<p>In image classification, a new method called <strong>NoProp</strong> showed that a neural network can be trained <strong>without any forward or backward propagation</strong> by having each layer learn to denoise a noisy target (here, a noisy label embedding) (<a href="https://medium.com" target="_blank" rel="noopener noreferrer">medium.com</a>, <a href="https://paperswithcode.com" target="_blank" rel="noopener noreferrer">paperswithcode.com</a>).</p>
<p>This report investigates the feasibility of extending such no-prop, diffusion-inspired training from simple labeling tasks to <strong>text generation</strong>. We review progress (2023-2025) on diffusion models for language, summarize no-propagation learning techniques, and assess how diffusion architectures might be adapted to support training without traditional backprop. We also discuss existing implementations, theoretical support, and key challenges, comparing them to conventional paradigms.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="diffusion-models-in-text-generation-background">Diffusion Models in Text Generation (Background)<a href="#diffusion-models-in-text-generation-background" class="hash-link" aria-label="Direct link to Diffusion Models in Text Generation (Background)" title="Direct link to Diffusion Models in Text Generation (Background)">​</a></h2>
<p>Diffusion models have emerged as a powerful class of generative models—first in image synthesis, and more recently in text.</p>
<p>In text generation they are typically used in a <strong>non-autoregressive (NAR)</strong> setting: the model generates an entire sequence through parallel or iterative refinement rather than token-by-token autoregression. A forward <em>noising</em> process gradually corrupts a text representation; a neural model is trained to reverse this process, transforming noise back into coherent text. A 2023 survey showed that introducing diffusion models into NAR text generation significantly narrows the gap with traditional autoregressive transformers (<a href="https://ar5iv.org" target="_blank" rel="noopener noreferrer">ar5iv.org</a>).</p>
<p>Two main paradigms exist:</p>
<ol>
<li><strong>Discrete diffusion</strong> (operating directly on token sequences)</li>
<li><strong>Continuous/embedding diffusion</strong> (adding noise in a continuous space)</li>
</ol>
<p>Embedding diffusion has proven more effective and flexible (<a href="https://aclanthology.org" target="_blank" rel="noopener noreferrer">ACL Anthology</a>). The workflow:</p>
<ol>
<li>Define an embedding for each token/sequence.</li>
<li>Add Gaussian noise over a series of timesteps.</li>
<li>Train a model to gradually remove the noise.</li>
</ol>
<p>Because the denoised output is continuous, an <strong>encoder–decoder bridge</strong> is required: map discrete tokens → embeddings → <strong>diffusion</strong> → denoised embeddings → tokens (e.g. via arg-max over a vocabulary projection). Regularization (e.g. the <em>anchor loss</em> in <strong>Difformer</strong>) prevents the embedding space from collapsing (<a href="https://aclanthology.org" target="_blank" rel="noopener noreferrer">ACL Anthology</a>).</p>
<p><strong>Key examples</strong></p>
<ul>
<li><strong>DiffuSeq</strong> (ICLR 2023): sequence-to-sequence diffusion for translation &amp; paraphrasing (<a href="https://github.com/diffuseq" target="_blank" rel="noopener noreferrer">GitHub</a>).</li>
<li><strong>Diffusion-LM</strong> (NeurIPS 2022): controllable text generation via latent-space diffusion (<a href="https://github.com/Diffusion-LM" target="_blank" rel="noopener noreferrer">GitHub</a>).</li>
<li><strong>FlowSeq / Flow matching</strong> (EACL 2024): treats generation as an ODE that transports noisy text embeddings to clean embeddings in a few large steps (<a href="https://aclanthology.org" target="_blank" rel="noopener noreferrer">ACL Anthology</a>).</li>
</ul>
<p>These advances have reduced required diffusion steps from hundreds to ∼10-20, making diffusion LMs practical. <strong>All</strong> are still trained with standard backprop—raising the question: <em>Can we train such models without backprop</em>?</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="no-propagation-learning-techniques-background">No-Propagation Learning Techniques (Background)<a href="#no-propagation-learning-techniques-background" class="hash-link" aria-label="Direct link to No-Propagation Learning Techniques (Background)" title="Direct link to No-Propagation Learning Techniques (Background)">​</a></h2>
<p><strong>No-prop</strong> (no-backprop) learning avoids global gradient propagation.</p>
<ul>
<li><strong>Forward-Forward (FF) algorithm</strong> (<a href="https://cs.toronto.edu" target="_blank" rel="noopener noreferrer">cs.toronto.edu</a>): two forward passes (positive vs. negative data) adjust weights via a local goodness measure.</li>
<li><strong>Target propagation</strong> (<a href="https://paperswithcode.com" target="_blank" rel="noopener noreferrer">paperswithcode.com</a>): propagate <em>targets</em> (desired activations) instead of gradients.</li>
<li><strong>Feedback alignment</strong>: random feedback weights replace exact gradient transposes.</li>
<li><strong>Zeroth-order (ZO) optimization</strong> (<a href="https://arxiv.org" target="_blank" rel="noopener noreferrer">arXiv</a>): evolutionary strategies / finite-difference updates using only forward passes.</li>
<li><strong>Greedy layer-wise training</strong>: train one layer (or block) at a time (e.g. stacked autoencoders).</li>
</ul>
<p>Despite decades of effort, none matched the ease/performance of backprop on large tasks—until <strong>NoProp</strong> (Li, Teh &amp; Pascanu 2025), a <strong>gradient-free method inspired by denoising diffusion</strong> (<a href="https://paperswithcode.com" target="_blank" rel="noopener noreferrer">paperswithcode.com</a>).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-noprop-works-in-image-classification">How NoProp Works in Image Classification<a href="#how-noprop-works-in-image-classification" class="hash-link" aria-label="Direct link to How NoProp Works in Image Classification" title="Direct link to How NoProp Works in Image Classification">​</a></h3>
<ol>
<li><strong>Label embedding space:</strong> each class label = learnable vector (not one-hot).</li>
<li><strong>Add Gaussian noise</strong> at multiple levels—one level per layer.</li>
<li><strong>Layer $t$</strong> receives the noisy label from layer $t!-!1$ plus the original image <em>x</em>, and learns to predict the clean label embedding (MSE loss).</li>
</ol>
<p>Because each layer is trained <strong>in isolation</strong> (fixed noise level, local loss), <strong>no forward/backward pass through the full network is needed</strong>. Layers could even train in parallel.</p>
<p><strong>Results:</strong> NoProp matched backprop accuracy on MNIST/CIFAR-10 with ≈ ½ GPU memory and faster training (<a href="https://medium.com" target="_blank" rel="noopener noreferrer">medium.com</a>). It outperformed previous no-backprop methods in both accuracy and simplicity—providing a blueprint for gradient-free training.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="adapting-diffusion-no-prop-techniques-to-text-generation">Adapting Diffusion No-Prop Techniques to Text Generation<a href="#adapting-diffusion-no-prop-techniques-to-text-generation" class="hash-link" aria-label="Direct link to Adapting Diffusion No-Prop Techniques to Text Generation" title="Direct link to Adapting Diffusion No-Prop Techniques to Text Generation">​</a></h2>
<p>Extending no-prop to text is promising yet challenging.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1--representation-of-text-for-diffusion">1  Representation of Text for Diffusion<a href="#1--representation-of-text-for-diffusion" class="hash-link" aria-label="Direct link to 1  Representation of Text for Diffusion" title="Direct link to 1  Representation of Text for Diffusion">​</a></h3>
<ul>
<li>Convert sentences to continuous embeddings (pretrained or learned).</li>
<li>Assign each layer a fixed noise level; train it to denoise toward clean embeddings (either one-step-to-clean or step-wise targets).</li>
<li>Decode final embeddings to tokens via a vocabulary projection. The decoder itself could be trained via a diffusion-style objective or separately with simple supervision.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2--model-architecture-for-text-denoising">2  Model Architecture for Text Denoising<a href="#2--model-architecture-for-text-denoising" class="hash-link" aria-label="Direct link to 2  Model Architecture for Text Denoising" title="Direct link to 2  Model Architecture for Text Denoising">​</a></h3>
<ul>
<li><strong>Unrolled Transformer decoder:</strong> stack <strong>T</strong> distinct blocks, each a diffusion step; give each block a timestep/noise embedding.</li>
<li>Train each block independently on (source context, noisy target) → clean target.</li>
<li>Parameter explosion for large T: mitigate via smaller blocks, shared encoders (if conditional), or future weight-tying research.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3--feasibility-evidence">3  Feasibility Evidence<a href="#3--feasibility-evidence" class="hash-link" aria-label="Direct link to 3  Feasibility Evidence" title="Direct link to 3  Feasibility Evidence">​</a></h3>
<ul>
<li><strong>Denoising score matching</strong> (theory behind diffusion) supports stacked local denoisers (<a href="https://arxiv.org" target="_blank" rel="noopener noreferrer">arXiv</a>).</li>
<li><strong>Flow matching</strong> &amp; diffusion-LM successes show unrolled, non-end-to-end training can work (<a href="https://aclanthology.org" target="_blank" rel="noopener noreferrer">ACL Anthology</a>).</li>
<li><strong>ZO fine-tuning</strong> proves large LLMs can update without gradients, hinting memory advantages for no-prop diffusion (<a href="https://arxiv.org" target="_blank" rel="noopener noreferrer">arXiv</a>).</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="4--key-challenges">4  Key Challenges<a href="#4--key-challenges" class="hash-link" aria-label="Direct link to 4  Key Challenges" title="Direct link to 4  Key Challenges">​</a></h3>
<ol>
<li><strong>Discrete data complexity</strong> → need robust embedding space (anchor losses, pretrained init).</li>
<li><strong>Sequential coherence</strong> → may require self-conditioning or iterative strategies.</li>
<li><strong>Scaling &amp; depth</strong> → many denoising steps increase parameters; continuous-time variants might share weights.</li>
<li><strong>Noise scheduling &amp; coordination</strong> → schedule design, layer-interaction, iterative refinement without backprop.</li>
<li><strong>Evaluation</strong> → global quality metrics for a stack of independently trained layers.</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5--comparisons-to-backprop-training">5  Comparisons to Backprop Training<a href="#5--comparisons-to-backprop-training" class="hash-link" aria-label="Direct link to 5  Comparisons to Backprop Training" title="Direct link to 5  Comparisons to Backprop Training">​</a></h3>
<ul>
<li><strong>Credit assignment:</strong> local (no-prop) vs. global (backprop) may yield different representation hierarchies.</li>
<li><strong>Optimization:</strong> many independent sub-problems vs. one joint problem; hybrids (independent pre-train + light joint fine-tune) are conceivable.</li>
<li><strong>Inference speed:</strong> fixed-T diffusion vs. token-by-token autoregression; pipeline parallelism possible.</li>
<li><strong>Reuse of pretrained knowledge:</strong> initializing layers from pretrained transformers or embeddings can bootstrap no-prop models.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="outlook-and-ongoing-work">Outlook and Ongoing Work<a href="#outlook-and-ongoing-work" class="hash-link" aria-label="Direct link to Outlook and Ongoing Work" title="Direct link to Outlook and Ongoing Work">​</a></h2>
<p>As of early 2025, no published no-prop text generator exists, but building blocks are ready:</p>
<ul>
<li>Small-scale proofs-of-concept (limited-vocabulary autoencoders) are likely next.</li>
<li>Improved local-loss algorithms (e.g. “Trifecta” stabilizers for forward-forward) will aid no-prop diffusion (<a href="https://medium.com" target="_blank" rel="noopener noreferrer">medium.com</a>).</li>
<li>Embedding-space safeguards (anchor loss) and step-distillation can transfer directly.</li>
<li>Biological inspiration: no-prop diffusion resembles cortical iterative denoising loops (<a href="https://cs.toronto.edu" target="_blank" rel="noopener noreferrer">cs.toronto.edu</a>).</li>
</ul>
<p>If engineering hurdles are cleared—scaling, coherence, efficiency—we could see new language models trained in a highly distributed, memory-light fashion, offering novel generalization properties. The next 1-2 years will be pivotal for backprop-free generative modeling, with text generation as a key benchmark.</p>
<hr></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a title="Superior Agents tag description" class="tag_zVej tagRegular_sFm0" href="/superioragents-docs/blog/tags/superioragents">Superior Agents</a></li><li class="tag_QGVx"><a title="Research tag description" class="tag_zVej tagRegular_sFm0" href="/superioragents-docs/blog/tags/research">Research</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/superioragents/superior-agents/edit/main/blog/blog/2025-05-15-adapting-diffusion-models/2025-05-15-adapting-diffusion-models.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/superioragents-docs/blog/2025-05-15-less-data-more-intelligence"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Less Data, More Intelligence: How Curated Training Data Unlocks LLM Power</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#diffusion-models-in-text-generation-background" class="table-of-contents__link toc-highlight">Diffusion Models in Text Generation (Background)</a></li><li><a href="#no-propagation-learning-techniques-background" class="table-of-contents__link toc-highlight">No-Propagation Learning Techniques (Background)</a><ul><li><a href="#how-noprop-works-in-image-classification" class="table-of-contents__link toc-highlight">How NoProp Works in Image Classification</a></li></ul></li><li><a href="#adapting-diffusion-no-prop-techniques-to-text-generation" class="table-of-contents__link toc-highlight">Adapting Diffusion No-Prop Techniques to Text Generation</a><ul><li><a href="#1--representation-of-text-for-diffusion" class="table-of-contents__link toc-highlight">1  Representation of Text for Diffusion</a></li><li><a href="#2--model-architecture-for-text-denoising" class="table-of-contents__link toc-highlight">2  Model Architecture for Text Denoising</a></li><li><a href="#3--feasibility-evidence" class="table-of-contents__link toc-highlight">3  Feasibility Evidence</a></li><li><a href="#4--key-challenges" class="table-of-contents__link toc-highlight">4  Key Challenges</a></li><li><a href="#5--comparisons-to-backprop-training" class="table-of-contents__link toc-highlight">5  Comparisons to Backprop Training</a></li></ul></li><li><a href="#outlook-and-ongoing-work" class="table-of-contents__link toc-highlight">Outlook and Ongoing Work</a></li></ul></div></div></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/superioragents-docs/docs/category/getting-started">Documentation</a></li><li class="footer__item"><a href="https://arxiv.org/abs/2504.04711" target="_blank" rel="noopener noreferrer" class="footer__link-item">arXiv Research Paper<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.gg/865JrDPU2J" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://x.com/Superior_Agents" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/superioragents-docs/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/SuperiorAgents/superior-agents" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 KIP Protocol</div></div></div></footer></div>
</body>
</html>