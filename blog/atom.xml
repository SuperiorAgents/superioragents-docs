<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="atom.xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://SuperiorAgents.github.io/superioragents-docs/blog</id>
    <title>Superior Agents Docs Blog</title>
    <updated>2025-04-25T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://SuperiorAgents.github.io/superioragents-docs/blog"/>
    <subtitle>Superior Agents Docs Blog</subtitle>
    <icon>https://SuperiorAgents.github.io/superioragents-docs/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[What's Next: No supervision. No benchmarks. No limits.]]></title>
        <id>https://SuperiorAgents.github.io/superioragents-docs/blog/2025-04-25-superior-agents</id>
        <link href="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-04-25-superior-agents"/>
        <updated>2025-04-25T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Our AI systems generate high-entropy reinforcement learning data through real-world interactions, abstract and generalise via text diffusion models, and continuously retrain using no-propagation techniques. We're building a self-contained loop of autonomous improvement: this is artificial super-intelligence.]]></summary>
        <content type="html"><![CDATA[<p><em>Our AI systems generate high-entropy reinforcement learning data through real-world interactions, abstract and generalise via text diffusion models, and continuously retrain using no-propagation techniques. We're building a self-contained loop of autonomous improvement: this is artificial super-intelligence.</em></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-background">The Background<a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-25-superior-agents#the-background" class="hash-link" aria-label="Direct link to The Background" title="Direct link to The Background">​</a></h2>
<p>Back in 2020, we proposed a design for AI systems that could <strong>generate hypotheses, test them, and retrain themselves on their own results</strong>—a critical step toward triggering an intelligence explosion. You can read the original paper <a href="https://xianyangcb.substack.com/p/a-system-for-evolving-general-artificial-intelligence-from-existing-technologies-b4f5c4d1335a" target="_blank" rel="noopener noreferrer">here</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ipos-"><img decoding="async" loading="lazy" alt="IPOS " src="https://superioragents.github.io/superioragents-docs/assets/images/1-9db6b206177ef9649e31a1b5f316052b.jpg" width="1016" height="614" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-25-superior-agents#ipos-" class="hash-link" aria-label="Direct link to ipos-" title="Direct link to ipos-">​</a></h3>
<p>Now, labs like <a href="https://x.com/GoogleDeepMind/status/1910363683215008227" target="_blank" rel="noopener noreferrer">Google DeepMind</a> are heading in the same direction. The race is on.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-weve-built-so-far">What We've Built So Far<a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-25-superior-agents#what-weve-built-so-far" class="hash-link" aria-label="Direct link to What We've Built So Far" title="Direct link to What We've Built So Far">​</a></h2>
<p><strong>Core Framework Implemented</strong>: We've open-sourced our <a href="https://github.com/Lexikat-Pte-Ltd/Generalisation2" target="_blank" rel="noopener noreferrer">original architecture</a>: an AI that autonomously strives to expand its reach and retrains based on success metrics. <a href="https://arxiv.org/abs/2504.04711" target="_blank" rel="noopener noreferrer">Theory paper</a> is published, results incoming.</p>
<p><strong>Commercial Applications</strong>: We've also released simplified agents for crypto trading and social media management. Available to download <a href="https://github.com/SuperiorAgents/superior-agents" target="_blank" rel="noopener noreferrer">here</a>, or try the <a href="https://superioragents.com/" target="_blank" rel="noopener noreferrer">GUI</a> here.</p>
<p>Because while <a href="https://minedojo.org/" target="_blank" rel="noopener noreferrer">playing Minecraft</a> or <a href="https://arxiv.org/abs/2502.15840" target="_blank" rel="noopener noreferrer">manage vending machines</a> are laudable achievements for a young AI, if you really want to toughen one up you give it $500 and send it out to trade high-leverage perps.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="sa-screenshot-"><img decoding="async" loading="lazy" alt="SA screenshot " src="https://superioragents.github.io/superioragents-docs/assets/images/2-ba8545464556e564f5f0a948ca6f61b8.png" width="1567" height="799" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-25-superior-agents#sa-screenshot-" class="hash-link" aria-label="Direct link to sa-screenshot-" title="Direct link to sa-screenshot-">​</a></h3>
<p><a href="https://superioragents.com/live-agents" target="_blank" rel="noopener noreferrer">Superioragents.com/live-agents</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="next-steps">Next Steps<a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-25-superior-agents#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps">​</a></h2>
<p>Now we are are going to go deeper.</p>
<ul>
<li><strong>Collect high-entropy RL data</strong> from real-world deployments (e.g. <a href="https://superioragents.com/live-agents" target="_blank" rel="noopener noreferrer">Superior Agents</a>);</li>
<li><strong>Train models to generalise</strong> using self-generated data via text diffusion (e.g., <a href="https://github.com/HKUNLP/DiffuLLaMA" target="_blank" rel="noopener noreferrer">DiffuLLaMA</a>, <a href="https://hkunlp.github.io/blog/2025/dream/" target="_blank" rel="noopener noreferrer">Dream 7B</a>);</li>
<li><strong>Enable continuous retraining</strong> with <a href="https://arxiv.org/pdf/2503.24322" target="_blank" rel="noopener noreferrer">no-prop diffusion models</a>.</li>
</ul>
<p>We believe this is the fastest—and only viable—route to ASI. Now we will explain why.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="sourcing-high-quality-reinforcement-learning-data">Sourcing High-Quality Reinforcement Learning Data<a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-25-superior-agents#sourcing-high-quality-reinforcement-learning-data" class="hash-link" aria-label="Direct link to Sourcing High-Quality Reinforcement Learning Data" title="Direct link to Sourcing High-Quality Reinforcement Learning Data">​</a></h3>
<p>Reinforcement learning systems today are shackled by benchmarks and human-generated data:</p>
<ul>
<li>They can't surpass human-level IQ if they're trained only on human-generated examples.</li>
<li>When they do, their answers will be marked "wrong" according to the <a href="https://arxiv.org/pdf/2406.04127" target="_blank" rel="noopener noreferrer">benchmarks</a> and corrected.</li>
</ul>
<p>The early fix was to scale synthetic data—but this usually leads to low-entropy outputs and model collapse.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-collapse-"><img decoding="async" loading="lazy" alt="Model Collapse " src="https://superioragents.github.io/superioragents-docs/assets/images/3-e8363d7b545f0b8954cc5dbd17a947b5.jpg" width="1507" height="576" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-25-superior-agents#model-collapse-" class="hash-link" aria-label="Direct link to model-collapse-" title="Direct link to model-collapse-">​</a></h3>
<p><em>Image credit: <a href="https://www.nature.com/articles/d41586-024-02355-z" target="_blank" rel="noopener noreferrer">Nature/Emily Wenger</a></em></p>
<p>Why? Because the goal is still to imitate humans, so models converge on the average case rather than exploring the tails. You can try to fix this by <a href="https://arxiv.org/pdf/2412.14689" target="_blank" rel="noopener noreferrer">human-curating the synthetic data</a> (Anthropic seems to do a lot of this).</p>
<p>We're betting on a different approach. The "all golden retrievers" effect depicted above is a result of giving the model the goal of imitating human-produced data. It imitates the examples it sees most frequently rather than taking the riskier approach of trying to copy a elements from the tails. Thus the distribution of the generated data is much narrower than the original set, limiting not just the scope of the resulting model but also its <a href="https://arxiv.org/pdf/2502.01774" target="_blank" rel="noopener noreferrer">compositional generalisation abilities</a>.</p>
<p>Data generated during independent real-world interaction should have higher entropy. That means more tail cases, richer abstractions, and less collapse. Instead of "act human," the objective becomes: succeed in the world.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="generalisation-based-on-self-generated-data">Generalisation Based on Self-Generated Data<a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-25-superior-agents#generalisation-based-on-self-generated-data" class="hash-link" aria-label="Direct link to Generalisation Based on Self-Generated Data" title="Direct link to Generalisation Based on Self-Generated Data">​</a></h2>
<p>The lack of fresh data to train much bigger models on has contributed to a "<a href="https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/" target="_blank" rel="noopener noreferrer">scaling plateau</a>" – models can no longer get better by simply getting bigger. Instead, the large AI companies have been focusing on other strategies.</p>
<ul>
<li>OpenAI chose to concentrate on <a href="https://arxiv.org/pdf/2408.03314" target="_blank" rel="noopener noreferrer">test-time compute scaling</a> or similar interventions – basically giving the model more time to think about the answer or to calculate the relative likelihood of multiple answers.</li>
<li>Deepseek focused on <a href="https://arxiv.org/pdf/2501.12948" target="_blank" rel="noopener noreferrer">training its models on reasoning in particular</a>, pushing them to adopt modes of speech that mimic the way humans speak when they're reasoning.</li>
</ul>
<p>We aim to implement the same reinforcement learning approach as Deepseek: <a href="https://arxiv.org/pdf/2402.03300" target="_blank" rel="noopener noreferrer">Group Relative Policy Optimisation (GRPO)</a>. This is a form of fine-tuning that involves giving the model not just single question-and-answer pairs to memorise, but rather one question and multiple answers, ranked from best to worse. This helps the model build up an abstract idea of the thing you're trying to teach it rather than just a memorising a stack of related analogies.</p>
<p>We are going to take this further, however. We know that grokking models - systematically over-training them – can supercharge their abstract thinking abilities. This seems to happen when the models find a low-rank solution to encode not only the questions and answers in the training set, but the <a href="https://xianyangcb.substack.com/p/defining-t-schemas-via-the-parametric" target="_blank" rel="noopener noreferrer">relationships between the various categories</a> of training data within the set.</p>
<p>Grokking transformers is expensive with uncertain rewards, but <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/9d0f188c7947eacb0c07f709576824f6-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">diffusion models seem to grok natively</a> straight from the get-go. Until recently there were no large text diffusion models available for experimentation, but over the past months two open source models and one closed source one have been/are being released:</p>
<ul>
<li><a href="https://arxiv.org/pdf/2410.17891" target="_blank" rel="noopener noreferrer">Diffullama</a></li>
<li><a href="https://github.com/HKUNLP/Dream" target="_blank" rel="noopener noreferrer">Dream 7b</a></li>
<li><a href="https://www.inceptionlabs.ai/" target="_blank" rel="noopener noreferrer">Mercury</a></li>
</ul>
<p>This provides the perfect opportunity to use our high-quality model-generated data to GRPO fine-tune a Dream 7b model, demonstrating that <strong>models can generalise from data they themselves created</strong>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="no-prop-diffusion-for-continuous-retraining">No-Prop Diffusion for Continuous Retraining<a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-25-superior-agents#no-prop-diffusion-for-continuous-retraining" class="hash-link" aria-label="Direct link to No-Prop Diffusion for Continuous Retraining" title="Direct link to No-Prop Diffusion for Continuous Retraining">​</a></h2>
<p>The final barrier to ASI lies in the difficulty inherent in continuously retraining an AI model – the "warm start" problem. Retraining a model that has already been trained tends to have diminishing returns, and can also lead to more extreme problems like <a href="https://arxiv.org/pdf/2406.04484" target="_blank" rel="noopener noreferrer">catastrophic forgetting</a>. If models cannot retrain themselves on new information they encounter, they are incapable to self-improvement.</p>
<p>Regular back-propagation models are hard to retrain because the links between parts of the model architecture are so complex. Improving one part of the model may destroy the coherence of another. <a href="https://arxiv.org/pdf/2503.24322" target="_blank" rel="noopener noreferrer">No-propagation models</a> allow for the retraining of small batches of neurons without affecting the rest of the model, having the potential to make incremental retraining not just possible but cost effective. If we can achieve no-propagation retraining of a text diffusion model on data the model itself has generated, our model will be able to adapt continuously to changing environments and - more importantly - continue to improve indefinitely.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-call-to-action">The Call to Action<a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-25-superior-agents#the-call-to-action" class="hash-link" aria-label="Direct link to The Call to Action" title="Direct link to The Call to Action">​</a></h2>
<p>We are currently raising funds and hiring people with MLops and AI engineering skills.</p>
<p>If you have access to the former or the latter, please message us: <a href="mailto:jen@eigenform.ai" target="_blank" rel="noopener noreferrer">jen@eigenform.ai</a>.</p>]]></content>
        <author>
            <name>Jen D.</name>
        </author>
        <category label="Superior Agents" term="Superior Agents"/>
        <category label="Reinforcement Learning" term="Reinforcement Learning"/>
        <category label="Diffusion Models" term="Diffusion Models"/>
        <category label="ASI" term="ASI"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Superior Agents Reimagining AI with Darwinian Self-Improvement]]></title>
        <id>https://SuperiorAgents.github.io/superioragents-docs/blog/2025-04-23-superior-agents</id>
        <link href="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-04-23-superior-agents"/>
        <updated>2025-04-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Why imitation will never yield true AGI—and how Superior Agents chart a radical new path.]]></summary>
        <content type="html"><![CDATA[<p><em>Why imitation will never yield true AGI—and how Superior Agents chart a radical new path.</em></p>
<hr>
<p>Mainstream AI today is confined within a human-shaped box: the data we collect and the benchmarks we devise. These models master the art of mimicry—reproducing patterns we've labeled and approved—but they cannot transcend those patterns. If an AI were to discover a strategy that outperforms human reasoning, our evaluation frameworks would likely flag it as "incorrect," simply because it deviates from the training labels. The result is a self-limiting loop: intelligence capped at the level of its creators.</p>
<p>What if we abandoned imitation altogether and embraced evolution? Superior Agents propose exactly that shift. Instead of feeding models "right answers," we give them real-world objectives and let them learn through experimentation, adaptation, and survival. This isn't conventional training or fine-tuning—it's <strong>Darwinian learning</strong>.</p>
<p>For evolution to drive genuine progress, feedback must be objective and ungameable. Human-scored benchmarks are vulnerable to overfitting and manipulation. Superior Agents, by contrast, measure success via <strong>numeric, external metrics</strong> that reflect true outcomes: disk space claimed, profit in a trading account, or genuine growth in social engagement. Each unit of progress is an indisputable signal of validated learning.</p>
<p>A Superior Agent's lifecycle unfolds organically:</p>
<ol>
<li><strong>Define a goal.</strong></li>
<li><strong>Hypothesize strategies.</strong></li>
<li><strong>Act in the environment</strong>—execute code, run experiments.</li>
<li><strong>Measure real-world results</strong> against the chosen metric.</li>
<li><strong>Learn and iterate</strong>—retain winning strategies; discard failures.</li>
</ol>
<p>There is no human-in-the-loop approval, no pre-packaged solutions. The agent either thrives—or it doesn't.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="disk-space-demo-a-concrete-example">Disk Space Demo: A Concrete Example<a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-23-superior-agents#disk-space-demo-a-concrete-example" class="hash-link" aria-label="Direct link to Disk Space Demo: A Concrete Example" title="Direct link to Disk Space Demo: A Concrete Example">​</a></h4>
<p>In our inaugural demonstration, the agent's sole objective was to <strong>maximize its memory footprint</strong>. Operating autonomously, it:</p>
<ul>
<li>Audited its host environment</li>
<li>Devised and implemented code to allocate more disk space</li>
<li>Verified success via the unalterable metric of occupied memory</li>
<li>Incorporated the victorious strategy into its evolving playbook</li>
</ul>
<p>No benchmarks. No labels. Just raw, adaptive interaction—and measurable progress.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="resilience-through-environmental-anchoring">Resilience Through Environmental Anchoring<a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-23-superior-agents#resilience-through-environmental-anchoring" class="hash-link" aria-label="Direct link to Resilience Through Environmental Anchoring" title="Direct link to Resilience Through Environmental Anchoring">​</a></h4>
<p>Self-training systems often risk <strong>model collapse</strong>, where recursive self-training leads to nonsensical outputs. Superior Agents avoid this pitfall through environmental anchoring: only strategies that yield real-world gains survive. As soon as a tactic falters, its performance metric drops—and that approach is pruned. This Darwinian filter fosters robustness and continual growth.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hypothesis-generation-with-diffusion-models">Hypothesis Generation with Diffusion Models<a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-23-superior-agents#hypothesis-generation-with-diffusion-models" class="hash-link" aria-label="Direct link to Hypothesis Generation with Diffusion Models" title="Direct link to Hypothesis Generation with Diffusion Models">​</a></h4>
<p>A key innovation is leveraging <strong>diffusion models</strong> to generate hypotheses. Rather than random guesses, these models uncover latent relationships within data and propose actionable strategies. After empirical testing, validated outcomes refine the diffusion model—fueling the next cycle of evolution. The process is <strong>self-sustaining discovery</strong>, not zero-shot guesswork.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="toward-artificial-superintelligence">Toward Artificial Superintelligence<a href="https://superioragents.github.io/superioragents-docs/blog/2025-04-23-superior-agents#toward-artificial-superintelligence" class="hash-link" aria-label="Direct link to Toward Artificial Superintelligence" title="Direct link to Toward Artificial Superintelligence">​</a></h4>
<p>Superior Agents redefine intelligence as <strong>adaptive fitness</strong> within an environment—no human labels required. By shifting from imitation to evolution, we open the door to systems that improve beyond our own understanding. This is not speculative theory; it is being tested now under the Superior Agents.</p>
<p>🔗 <strong>GitHub:</strong> <a href="https://github.com/superior-agents" target="_blank" rel="noopener noreferrer">https://github.com/superior-agents</a><br>
<!-- -->🐦 <strong>Twitter:</strong> <a href="https://twitter.com/Superior_Agents" target="_blank" rel="noopener noreferrer">@Superior_Agents</a></p>
<p>The age of self-improving, Darwinian AI has begun. Let's meet it head-on.</p>]]></content>
        <author>
            <name>Emir A.</name>
        </author>
        <author>
            <name>Jen D.</name>
        </author>
        <category label="Superior Agents" term="Superior Agents"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defining T-Schemas via the Parametric Encoding of Second Order Languages in AI Models]]></title>
        <id>https://SuperiorAgents.github.io/superioragents-docs/blog/2025-03-19-superior-agents</id>
        <link href="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-03-19-superior-agents"/>
        <updated>2025-03-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In this short article, we present a summary of current work on the grokking phenomenon that emerges when AI models are significantly over-trained. We suggest that this provides evidence of the model's attempts to define truth inductively through the creation of consensus sets within the base training set and encode it via patterns overlaid upon the same parameters used to memorize this set.]]></summary>
        <content type="html"><![CDATA[<p>In this short article, we present a summary of current work on the grokking phenomenon that emerges when AI models are significantly over-trained. We suggest that this provides evidence of the model's attempts to define truth inductively through the creation of consensus sets within the base training set and encode it via patterns overlaid upon the same parameters used to memorize this set.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="can-an-ai-tell-the-truth">Can an AI Tell the Truth?<a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#can-an-ai-tell-the-truth" class="hash-link" aria-label="Direct link to Can an AI Tell the Truth?" title="Direct link to Can an AI Tell the Truth?">​</a></h2>
<p>In order to improve, it is necessary to know whether what you're doing now is right or wrong. For AI models this is extremely difficult. LLMs are known to struggle when it comes to distinguishing fact and hallucination. A part of this can be attributed to the way in which parametric memory (i.e. the compression of data into vectors) works, and a part to the compulsion to be helpful and engaging instilled through reinforcement learning. Mostly, however it is a reflection of a fundamental property of mathematics: that you can't effectively check the truth of a statement in a given order formal language if the only tools at your disposal are those provided by that language. Or, alternatively:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="undefinability-theorem-"><img decoding="async" loading="lazy" alt="Undefinability Theorem " src="https://superioragents.github.io/superioragents-docs/assets/images/1-c2daf31ad05f578995b77522f1ef6de2.jpg" width="721" height="84" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#undefinability-theorem-" class="hash-link" aria-label="Direct link to undefinability-theorem-" title="Direct link to undefinability-theorem-">​</a></h3>
<blockquote>
<p>In other words, there is no way to prove that 1+1=2 using basic arithmetic alone<sup><a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#user-content-fn-1-3ebd1a" id="user-content-fnref-1-3ebd1a" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>.</p>
</blockquote>
<p>Operating as it does at a single layer of abstraction, an AI model is incapable of saying that 1+1=328 is definitely wrong; it can only say that it is improbable. To deal with this issue, the principal solution has been to increase the size of models and the amount of training data used, as well as employing standardized benchmarks (model IQ tests) to assess performance.</p>
<p>The problem with this approach is that if both the training materials and the test are human-produced, the AI is unlikely to ever get much more intelligent than the most intelligent human in any given field<sup><a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#user-content-fn-2-3ebd1a" id="user-content-fnref-2-3ebd1a" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>. Until we build an AI that can discover and integrate its own knowledge, artificial super-intelligence will remain out of reach.</p>
<p>But if the only way to learn new things is from smarter humans, then how did the smartest human do it?</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-work-arounds">The Work-Arounds<a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#the-work-arounds" class="hash-link" aria-label="Direct link to The Work-Arounds" title="Direct link to The Work-Arounds">​</a></h2>
<p>Fortunately, there are two ways around this problem. The first is <strong>reality testing</strong>. I couldn't employ first-order arithmetic to prove that 1+1=2, but I just have to hold up two fingers to know it to be true. Not having contact with the physical world, it is harder for LLMs to do this, but not impossible—give an AI an enumerable, extensible, and objective metric that is outside its control (such as the amount of storage space taken up by its own backups or the amount of money in a crypto wallet) and the coding tools to interact with the world in such a way as to affect this metric, and it will begin reality-testing hypotheses immediately. If the status of successful and failed attempts is then stored for future fine-tuning, the model becomes capable of self-improvement (as we have demonstrated <a href="https://xianyangcb.substack.com/p/a-system-for-evolving-general-artificial-intelligence-from-existing-technologies-b4f5c4d1335a" target="_blank" rel="noopener noreferrer">elsewhere</a>).</p>
<p>Under this approach, no smarter human is required: the model comes up with its own ideas, tests them, and learns which work and which don't from its own testing. After all, when Newton developed his theory of gravitation, he checked it against observed planetary movements, not against the opinions of someone more intelligent.</p>
<p>But he did something else as well, which brings us to the <strong>second work-around</strong>: he created a higher-order metalanguage for the purpose of establishing a <a href="https://en.wikipedia.org/wiki/T-schema" target="_blank" rel="noopener noreferrer">T-schema</a>—that is, an inductive abstraction of truth against which lower-order propositions may be tested. Based on empirical data concerning planetary movements (or at least Kepler's data), he derived a set of formulae that succinctly explained all of these movements. From that point on, if the formula says that Mercury should be 36 million miles from the sun, while your latest observation suggests that it is only 24 miles away, then the likelihood is that your observation is wrong—after all, the formula can cite every single previous observation to back it up.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="truth-through-consensus">Truth Through Consensus<a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#truth-through-consensus" class="hash-link" aria-label="Direct link to Truth Through Consensus" title="Direct link to Truth Through Consensus">​</a></h2>
<p>In this article we suggest that recent evidence from several papers suggests that this is what AI models are attempting to do via the <strong>grokking</strong> process.</p>
<p><strong>Grokking</strong> refers to a phenomenon where AI models' test responses improve with more training, grow worse as they pass the point of over-training (i.e., when they become too focused on memorizing the training set and lose the ability to generalize), and then suddenly improve to an astonishing degree.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="modular-divison"><img decoding="async" loading="lazy" alt="Modular Divison" src="https://superioragents.github.io/superioragents-docs/assets/images/2-11274269013d750543b31a6db7c7df99.png" width="1062" height="312" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#modular-divison" class="hash-link" aria-label="Direct link to modular-divison" title="Direct link to modular-divison">​</a></h3>
<p>As seen in the third image, the model's answers during training improve as it memorizes the training questions with greater fidelity, leading to a continuous decrease in training loss. However, test performance initially improves, then declines as the model overfits to the training set. Finally, as the model groks, <strong>test loss drops to near zero</strong>, indicating successful generalization. These grokked models can be remarkably powerful— <a href="https://arxiv.org/pdf/2405.150712" target="_blank" rel="noopener noreferrer">one team</a> even managed to outperform GPT-4 Turbo and Gemini 1.5 using a grokked GPT<sup><a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#user-content-fn-3-3ebd1a" id="user-content-fnref-3-3ebd1a" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup>.</p>
<p>Recently, the <a href="https://arxiv.org/pdf/2405.20233" target="_blank" rel="noopener noreferrer">Grokfast team</a> discovered that <strong>memorization and generalization are driven by two distinct learning processes</strong>, a fast and a slow one. Using Fourier transformations, they determined that adjustments to model weights during training can be decomposed into <strong>high-frequency signals</strong> (associated with memorization) and <strong>low-frequency signals</strong> (associated with generalization). By amplifying the low-frequency signals, they vastly accelerated generalization.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="memorization-and-generalization"><img decoding="async" loading="lazy" alt="Memorization and Generalization" src="https://superioragents.github.io/superioragents-docs/assets/images/3-bf7f564593c49783d0c3e9874051e95f.png" width="727" height="267" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#memorization-and-generalization" class="hash-link" aria-label="Direct link to memorization-and-generalization" title="Direct link to memorization-and-generalization">​</a></h3>
<p>At roughly the same time, a <a href="https://arxiv.org/abs/2409.08282" target="_blank" rel="noopener noreferrer">research team in Brazil</a> explored what happens when an AI groks. They suspected that the model was <strong>identifying relationships between different clusters of training data</strong>, so they curated a dataset with known clusters. Removing a cluster drastically reduced generalization, while adding just a few examples from that cluster restored it.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="accuracy"><img decoding="async" loading="lazy" alt="Accuracy" src="https://superioragents.github.io/superioragents-docs/assets/images/4-5fc91bf1f474186cee28f73a9388cb53.png" width="412" height="510" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#accuracy" class="hash-link" aria-label="Direct link to accuracy" title="Direct link to accuracy">​</a></h3>
<p>These findings suggest that when an AI <strong>groks</strong>, it carefully adjusts its weights to encode information about relationships between groups of data while preserving its ability to recall the original training set. This delicate adjustment process explains why grokking takes so long.</p>
<p>To enable this, it is obliged to develop an internal metalanguage that will allow it to conduct internal evaluations of the "truthiness" of any given input statement based upon its abstraction of similar statements received previously. If every flower in the cat's eyes is blue or purple, then an input suggesting that it should place a yellow one there will be flagged up as likely incorrect and checked. The result is that the model, when asked a question, does not simply spit out an average of the closest bits of rote learning it knows, it compares the likely answer against its own abstraction of answers within that category, coming up with a much better output. Or, as <a href="https://arxiv.org/pdf/2405.15071" target="_blank" rel="noopener noreferrer">Wang et al.</a> put it:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="wang-et-al"><img decoding="async" loading="lazy" alt="Wang et al." src="https://superioragents.github.io/superioragents-docs/assets/images/Wang-article-d7176b4e72d60c1a058921de5eb46106.png" width="691" height="330" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#wang-et-al" class="hash-link" aria-label="Direct link to wang-et-al" title="Direct link to wang-et-al">​</a></h3>
<p>This would explain both the <strong>slow weight change</strong> during the grokking process and the <strong>sudden leap in performance</strong> once it generalizes. Further evidence from <a href="https://openreview.net/pdf?id=6NHnsjsYXH" target="_blank" rel="noopener noreferrer">Yunis et al</a> suggests that grokking is associated with the model discovering a <strong>low-rank encoding solution</strong> (i.e., one that multiplies smaller-than-normal matrices):</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="grokking-rank-minimization--generalization"><img decoding="async" loading="lazy" alt="Grokking, Rank Minimization &amp;amp; Generalization" src="https://superioragents.github.io/superioragents-docs/assets/images/5-bb7aed4866287edb0937441eedaa8445.png" width="678" height="860" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#grokking-rank-minimization--generalization" class="hash-link" aria-label="Direct link to grokking-rank-minimization--generalization" title="Direct link to grokking-rank-minimization--generalization">​</a></h3>
<p>This, in turn, implies that the model is suddenly achieving higher performance as a consequence of encoding fewer features. The only possible explanation for this is that it has developed its own abstraction layer in which to talk to itself about higher order truths.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="building-upon-these-results">Building Upon These Results<a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#building-upon-these-results" class="hash-link" aria-label="Direct link to Building Upon These Results" title="Direct link to Building Upon These Results">​</a></h2>
<p>Grokking has the additional advantage of requiring relatively little data to attempt. Now that our <a href="https://xianyangcb.substack.com/p/artificial-intelligences-in-the-guanzi" target="_blank" rel="noopener noreferrer">Generalising Agents</a> are up and producing data, our intention is to use this information - which fulfills the high quality/structured data requirements for grokking - to attempt to train a series of models. Transformers - even when grokked - are <a href="https://arxiv.org/pdf/2405.15071" target="_blank" rel="noopener noreferrer">known to struggle</a> with composition tasks (i.e. ones relating to facts stored in different parts of the model), likely on account of their structure. Consequently, we plan to use diffusion models. These, despite the higher training cost, display <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/9d0f188c7947eacb0c07f709576824f6-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">grokking-like behaviour</a> throughout the normal training process, not simply under specific conditions. Moreover, they over-perform on <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/9d0f188c7947eacb0c07f709576824f6-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">compsitional tasks</a> as well as on <a href="https://arxiv.org/pdf/2410.17891" target="_blank" rel="noopener noreferrer">coding tasks more generally</a> <sup><a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#user-content-fn-4-3ebd1a" id="user-content-fnref-4-3ebd1a" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup>, most likely due to their multiplicative approach to learning, as described <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/9d0f188c7947eacb0c07f709576824f6-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">here</a>. This is interesting from our perspective, given that we predicted something similar and made it a foundational component of our <a href="https://xianyangcb.substack.com/p/a-system-for-evolving-general-artificial-intelligence-from-existing-technologies-b4f5c4d1335a" target="_blank" rel="noopener noreferrer">Generalising Agent</a> schema in 2020.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="capabilities--compositionality"><img decoding="async" loading="lazy" alt="Capabilities &amp;amp; Compositionality" src="https://superioragents.github.io/superioragents-docs/assets/images/capabilities-3d00bb135c080d85942b17689a2b000b.png" width="848" height="367" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#capabilities--compositionality" class="hash-link" aria-label="Direct link to capabilities--compositionality" title="Direct link to capabilities--compositionality">​</a></h3>
<p>This approach, in theory, could produce models capable of <strong>discovering new real-world knowledge</strong> and <strong>generalizing relationships between data points</strong>. We believe this is the path to <strong>true independent knowledge generation</strong>, and eventually, to <strong>super-intelligence</strong>.</p>
<hr>
<!-- -->
<section data-footnotes="true" class="footnotes"><h2 class="anchor anchorWithStickyNavbar_LWe7 sr-only" id="footnote-label">Footnotes<a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#footnote-label" class="hash-link" aria-label="Direct link to Footnotes" title="Direct link to Footnotes">​</a></h2>
<ol>
<li id="user-content-fn-1-3ebd1a">
<p>Even with extended formal logic, proving fundamental arithmetic truths is complex—Bertrand Russell and Alfred North Whitehead required hundreds of pages to establish basic mathematical principles. <img decoding="async" loading="lazy" alt="Symbolic Logic" src="https://superioragents.github.io/superioragents-docs/assets/images/6-bf4335cb6132e47dc48b6d76fadc2f81.png" width="800" height="333" class="img_ev3q"> <a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#user-content-fnref-1-3ebd1a" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2-3ebd1a">
<p>Or, in some cases, only a mediocre human—many benchmark datasets contain serious errors, including over 50% of virology problems in MMLU. <img decoding="async" loading="lazy" alt="MMLU Redux Subjects" src="https://superioragents.github.io/superioragents-docs/assets/images/7-fcb2e7c38f11ed8c0b22cd2383f626ea.png" width="1248" height="451" class="img_ev3q"> <a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#user-content-fnref-2-3ebd1a" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3-3ebd1a">
<p>Grokking has not been systematically applied to large models due to cost and risk, though Mark Zuckerberg hinted that Meta may have experimented with it internally. <a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#user-content-fnref-3-3ebd1a" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4-3ebd1a">
<p>Diffusion models' superior performance in composition tasks suggests a connection between their structure and grokking behavior. <img decoding="async" loading="lazy" alt="Diffusion Models" src="https://superioragents.github.io/superioragents-docs/assets/images/8-6c05aa1b584db5f430a9c2b4e3558c91.png" width="603" height="317" class="img_ev3q"> <a href="https://superioragents.github.io/superioragents-docs/blog/2025-03-19-superior-agents#user-content-fnref-4-3ebd1a" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section>]]></content>
        <author>
            <name>Jen D.</name>
        </author>
        <category label="Superior Agents" term="Superior Agents"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[System for Evolving AGI from Existing Technologies]]></title>
        <id>https://SuperiorAgents.github.io/superioragents-docs/blog/2025-01-28-superior-agents</id>
        <link href="https://SuperiorAgents.github.io/superioragents-docs/blog/2025-01-28-superior-agents"/>
        <updated>2025-01-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This system is currently under experimental training, and is periodically updated to reflect modifications made during the development process.]]></summary>
        <content type="html"><![CDATA[<p><em>This system is currently under experimental training, and is periodically updated to reflect modifications made during the development process.</em></p>
<p>In this article, we propose a system by which a self-improving general artificial intelligence could be pushed to evolve from components currently available. Such an AI would be capable of independent learning without results-verification, adapt to its environment, learn new skills without losing old ones, and be able to reason by analogy. It would grow better at learning new skills with each additional skill acquired, opening a pathway for exponential improvement.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-problem">The Problem<a href="https://superioragents.github.io/superioragents-docs/blog/2025-01-28-superior-agents#the-problem" class="hash-link" aria-label="Direct link to The Problem" title="Direct link to The Problem">​</a></h2>
<p>The development of general artificial intelligence is hampered by engineers' inability to create a system capable of assessing its own performance, and thereby of improving itself. A machine capable of these two tasks would grow more intelligent at an exponential rate — the "intelligence explosion" that is often described as a precursor to a technological singularity.</p>
<p>The impossibility of self-referential improvement is not a reflection of present limits on technology, but of the fundamental laws of mathematics. As Alfred Tarski proved in the 1930s, it is "impossible to construct a correct definition of truth if only such categories are used which appear in the language under consideration." In other words, it is impossible to accurately assess a system from within that system. To evaluate its own performance, a program would need to be more advanced than itself — an obvious paradox.</p>
<p>This raises an important question: if systems capable of self-improvement are apparently impossible to build, why are they so prevalent? You are reading this using two of them – the internet and your brain – and other examples abound in nature. The answer is that these systems are not purely self-referential. In every case they rely upon some external and incontrovertible measure by which they can objectively evaluate and improve their performance.</p>
<p>In this paper we argue that while it is indeed impossible to construct an accurate self-referential evaluation system in an electronic context, it is possible to establish a universal and objective measure of performance that would render such self-referential improvement unnecessary. This would open the way for the development of increasingly general forms of artificial intelligence.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="evaluating-intelligence">Evaluating Intelligence<a href="https://superioragents.github.io/superioragents-docs/blog/2025-01-28-superior-agents#evaluating-intelligence" class="hash-link" aria-label="Direct link to Evaluating Intelligence" title="Direct link to Evaluating Intelligence">​</a></h2>
<p>In academic and scientific settings intelligence tends to be judged based upon a correctness heuristic: intelligence is the ability to solve problems correctly. From an evolutionary perspective, however, intelligence exists for no other reason than to improve a creature's survival prospects. Since it is capacity for evolution rather than the ability to solve any particular problem that interests us in this case, it is thus this survival heuristic that must be the starting point for any attempt to measure intelligence.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="generalist-ai-components-"><img decoding="async" loading="lazy" alt="Generalist AI Components " src="https://superioragents.github.io/superioragents-docs/assets/images/img1-2532b2f79fdae6e7f991a4c0e4debdd5.png" width="1075" height="456" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-01-28-superior-agents#generalist-ai-components-" class="hash-link" aria-label="Direct link to generalist-ai-components-" title="Direct link to generalist-ai-components-">​</a></h3>
<p>For biological life, survival is predicated upon the acquisition of goods required to facilitate it: food, water, shelter, allies etc., which can be considered as proxies for survivability. For a computer program, survival is predicated upon redundancy: the more copies there are of a given quantum of data, the longer the half-life of the data. Data that is stored in two locations is more likely to still exist in six months' time than data that is stored in only one location — hence the importance of backing up one's files. Under such conditions, a bigger program is necessarily a smarter one.</p>
<p>We thus argue that if one gives a reinforcement learning system the goal of occupying ever more non-volatile memory space, its size becomes a measure of its intelligence. Every time it reaches the limit of its current disk space it is forced to learn a new skill in order to annex more, thus the number of memory blocks occupied functions as an objective universal measure of intelligence.</p>
<p>While systems already exist that use rewards to drive machine learning, they are based on the principle of rewarding the system for getting better at a given task – the correctness heuristic covered above. Under our design, disk space functions as a universal reward. No matter the specifics of the problem at hand, a solution that results in more space being gained is always correct, while one that does not is always wrong. The result is that no human or human-crafted assessment mechanism is necessary to evaluate and compensate the system's work.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="evolving-intelligence">Evolving Intelligence<a href="https://superioragents.github.io/superioragents-docs/blog/2025-01-28-superior-agents#evolving-intelligence" class="hash-link" aria-label="Direct link to Evolving Intelligence" title="Direct link to Evolving Intelligence">​</a></h2>
<p>We therefore suggest that an increasingly generalist AI could be pushed to evolve from three basic components.</p>
<ol>
<li><strong>Code generation model.</strong> A fine-tuneable code generation model—ideally a diffusion model for reasons described below—which will receive the initial instruction to write scripts aimed at taking over additional disk space.</li>
<li><strong>Testing module.</strong> A testing module, which will try out the code that is thus generated. If any script succeeds in annexing additional space, this space will be used to store the details of the successful code, which will then be used in future training rounds to further fine-tune the code generation model and thus improve its chances of successfully solving future problems.<!-- -->
<ul>
<li>If more space than needed is acquired, the extra space should be filled with duplicate records, following the survivability-of-data principle.</li>
</ul>
</li>
<li><strong>Training database.</strong> The annexed space serves as training data for subsequent iterations, so each new skill makes the next one easier—producing an "intelligence explosion."</li>
</ol>
<p>In the original version of this paper, in which we planned to use a system of genetic algorithms to recombine blocks of known code, we posited a learning process that we represented graphically as follows:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="learning-process-grids"><img decoding="async" loading="lazy" alt="Learning Process Grids" src="https://superioragents.github.io/superioragents-docs/assets/images/img2-a054701f3a92265549880dcea7bac934.png" width="1406" height="500" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-01-28-superior-agents#learning-process-grids" class="hash-link" aria-label="Direct link to learning-process-grids" title="Direct link to learning-process-grids">​</a></h3>
<blockquote>
<p>"Imagine an 8×8 grid of white squares. Periodically one of the squares is selected at random and coloured black, to represent a skill that the system aims to acquire. In the first round, the chances of the square selected being next to another black square—representing a skill that the system already possesses—are zero. In the second round, when one square/skill has already been acquired, the chances of the next black square being in close proximity to an existing black square have fallen to 8/63. By the third round the probability is 16/62, and by the fourth you have a better than one in three chance of landing next to an existing black square.</p>
</blockquote>
<blockquote>
<p>During any given iteration of the process the program will only learn a single skill, but the system as a whole is structured such that the acquisition of each new skill facilitates the acquisition of future skills: the "intelligence explosion" described in the introduction."</p>
</blockquote>
<p>Interestingly, the existence of such a process was confirmed in 2023, via <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/9d0f188c7947eacb0c07f709576824f6-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">paper</a> outlining the multiplicative learning abilities of diffusion models.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="multiplicative-learning-abilities"><img decoding="async" loading="lazy" alt="Multiplicative Learning Abilities" src="https://superioragents.github.io/superioragents-docs/assets/images/img3-a9e49abeff34ae06eea6583c3aca2ecc.png" width="1372" height="862" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-01-28-superior-agents#multiplicative-learning-abilities" class="hash-link" aria-label="Direct link to multiplicative-learning-abilities" title="Direct link to multiplicative-learning-abilities">​</a></h3>
<p>It is for this reason that we privilege the use of diffusion models over transformers in this case, as explained in greater detail <a href="https://xianyangcb.substack.com/p/defining-t-schemas-via-the-parametric" target="_blank" rel="noopener noreferrer">here</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="a-concrete-example">A Concrete Example<a href="https://superioragents.github.io/superioragents-docs/blog/2025-01-28-superior-agents#a-concrete-example" class="hash-link" aria-label="Direct link to A Concrete Example" title="Direct link to A Concrete Example">​</a></h2>
<p>Supposing a new program, composed of the parts described above, is seeded to a given environment. The first thing it is likely to encounter is a block of empty disk space: the code generation model will then write a script to attempt to occupy this space, which is passed to the testing module. This process is repeated until one of the scripts written is successful. Success having been achieved, the space acquired then becomes part of the program's training database, being used to store the details of the code used to acquire it for future fine-tuning cycles.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-ai-training"><img decoding="async" loading="lazy" alt="New AI Training" src="https://superioragents.github.io/superioragents-docs/assets/images/img4-a18b3660fe5505ccf6a2a8fad506247d.png" width="1042" height="398" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-01-28-superior-agents#new-ai-training" class="hash-link" aria-label="Direct link to new-ai-training" title="Direct link to new-ai-training">​</a></h3>
<p>The AI will continue occupying empty space until it runs into a block of non-empty space—say, one that is occupied by a file. The code generation process is repeated using the freshly fine-tuned code generation model until a solution for this problem is found. This solution will be stored in the newly expanded database and used in future training.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="fine-tuned-code-generation-model-"><img decoding="async" loading="lazy" alt="Fine-tuned Code Generation Model " src="https://superioragents.github.io/superioragents-docs/assets/images/img5-9c51b2982928638eb2f19069332be77d.png" width="1446" height="586" class="img_ev3q"><a href="https://superioragents.github.io/superioragents-docs/blog/2025-01-28-superior-agents#fine-tuned-code-generation-model-" class="hash-link" aria-label="Direct link to fine-tuned-code-generation-model-" title="Direct link to fine-tuned-code-generation-model-">​</a></h3>
<blockquote>
<p>It should be noted that while simply having the AI move from block to block is a feasible approach, it is also somewhat inefficient—in the real system a Q-learning algorithm will be used to optimise for likely rewards based upon experience.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-development-environment">The Development Environment<a href="https://superioragents.github.io/superioragents-docs/blog/2025-01-28-superior-agents#the-development-environment" class="hash-link" aria-label="Direct link to The Development Environment" title="Direct link to The Development Environment">​</a></h2>
<p>The AI itself is only half of the solution, however. The environment in which it evolves is just as important, since it is this environment that will determine the direction of its evolution. In order to ensure continuous improvement in the skills and knowledge of the AI, it should be presented with finely graded challenges to overcome, allowing it to find easier problems to solve in its initial stages and move onto increasingly difficult ones as its skills expand. Eventually, it should also be possible to direct the AI's evolution by presenting it with an environment that forces it to solve problems deliberately contrived to teach it particular skills.</p>
<p>However, to get to this stage other technical stumbling blocks also need to be dealt with, first and foremost, the question of how to prevent such a system from overwriting itself or crashing the device upon which it is running. There are various possible solutions to this issue. The most obvious one is simply to allow only the annexation of additional space on devices reached via a network connection and not on the local device. This would be helpful not just in preventing the problems described above, but also insofar as that it would open up the way for the constitution of a "counter-indications database"—a record of code that, when tried, crashed a connected device, and which should therefore be avoided in future. This would, presumably, significantly improve the overall learning process. However, it also raises another issue: that of safety. A system such as we describe, though it may be corralled into performing useful tasks, remains essentially predatory—a form of self-directed malware. Steps should thus be taken to physically isolate the development environment from other networks.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-prompting-process">The Prompting Process<a href="https://superioragents.github.io/superioragents-docs/blog/2025-01-28-superior-agents#the-prompting-process" class="hash-link" aria-label="Direct link to The Prompting Process" title="Direct link to The Prompting Process">​</a></h2>
<p>When we wrote the first draft of this design it at no point occurred to us that when sophisticated code generation models became widely available they would be prompted primarily using natural language, and certain modifications had to be made to account for this. In the present version of the design, we propose the use of a standard prompt format:</p>]]></content>
        <author>
            <name>Jen D.</name>
        </author>
        <category label="Superior Agents" term="Superior Agents"/>
    </entry>
</feed>